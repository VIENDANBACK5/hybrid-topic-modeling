#!/usr/bin/env python3
"""
Script ƒë·ªÉ fetch newspaper data t·ª´ external API
S·ª≠ d·ª•ng endpoint m·ªõi v·ªõi t√≠nh nƒÉng ph√¢n trang v√† deduplication
"""
import requests
import json
import sys
from typing import Optional

# API Configuration
API_BASE_URL = "http://localhost:7777"
EXTERNAL_API_URL = "http://192.168.30.28:8000/api/v1/posts/by-type/newspaper"


def fetch_newspaper_raw_all(
    page_size: int = 100,
    max_pages: Optional[int] = None
) -> dict:
    """
    Fetch T·∫§T C·∫¢ data (k·ªÉ c·∫£ duplicate) - kh√¥ng filter g√¨ c·∫£
    """
    url = f"{API_BASE_URL}/api/data/fetch-newspaper-raw-all"
    
    payload = {
        "base_api_url": EXTERNAL_API_URL,
        "page_size": page_size,
        "max_pages": max_pages,
        "sort_by": "id",
        "order": "desc"
    }
    
    print("üì∞ Fetching ALL newspaper data (no deduplication)...")
    print(f"   API: {EXTERNAL_API_URL}")
    print(f"   Page size: {page_size}")
    print(f"   Max pages: {max_pages or 'unlimited'}")
    print()
    
    response = requests.post(url, json=payload)
    response.raise_for_status()
    
    return response.json()


def fetch_newspaper_only(
    page_size: int = 100,
    max_pages: Optional[int] = None
) -> dict:
    """
    Fetch v√† filter duplicate v·ªõi DB, ch·ªâ l∆∞u data m·ªõi
    """
    url = f"{API_BASE_URL}/api/data/fetch-newspaper"
    
    payload = {
        "base_api_url": EXTERNAL_API_URL,
        "page_size": page_size,
        "max_pages": max_pages,
        "sort_by": "id",
        "order": "desc"
    }
    
    print("üì∞ Fetching newspaper data...")
    print(f"   API: {EXTERNAL_API_URL}")
    print(f"   Page size: {page_size}")
    print(f"   Max pages: {max_pages or 'unlimited'}")
    print()
    
    response = requests.post(url, json=payload)
    response.raise_for_status()
    
    return response.json()


def fetch_newspaper_full_etl(
    page_size: int = 100,
    max_pages: Optional[int] = None,
    update_existing: bool = False
) -> dict:
    """
    Fetch v√† ch·∫°y FULL ETL: Fetch ‚Üí Process ‚Üí Load to DB
    """
    url = f"{API_BASE_URL}/api/data/fetch-newspaper-full-etl"
    
    params = {"update_existing": update_existing}
    
    payload = {
        "base_api_url": EXTERNAL_API_URL,
        "page_size": page_size,
        "max_pages": max_pages,
        "sort_by": "id",
        "order": "desc"
    }
    
    print("üîÑ Running FULL ETL for newspaper data...")
    print(f"   API: {EXTERNAL_API_URL}")
    print(f"   Page size: {page_size}")
    print(f"   Max pages: {max_pages or 'unlimited'}")
    print(f"   Update existing: {update_existing}")
    print()
    
    response = requests.post(url, json=payload, params=params)
    response.raise_for_status()
    
    return response.json()


def print_results(result: dict):
    """In k·∫øt qu·∫£"""
    print("\n" + "="*60)
    print(f"Status: {result['status']}")
    print(f"Message: {result.get('message', 'N/A')}")
    
    if 'statistics' in result:
        print("\nüìä Statistics:")
        stats = result['statistics']
        for key, value in stats.items():
            print(f"   {key}: {value}")
    
    if 'results' in result and 'steps' in result['results']:
        print("\nüìã Pipeline Steps:")
        for step in result['results']['steps']:
            print(f"   {step['name']}: {step['status']}")
            if 'statistics' in step:
                for key, value in step['statistics'].items():
                    print(f"      {key}: {value}")
    
    if 'raw_file' in result:
        print(f"\nüíæ Raw file: {result['raw_file']}")
    
    print("="*60)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Fetch newspaper data from external API")
    parser.add_argument(
        "--mode",
        choices=["fetch-all", "fetch-only", "full-etl"],
        default="full-etl",
        help="Mode: fetch-all (l·∫•y h·∫øt, kh√¥ng filter), fetch-only (ch·ªâ fetch data m·ªõi), full-etl (fetch + process + load)"
    )
    parser.add_argument(
        "--page-size",
        type=int,
        default=100,
        help="S·ªë records m·ªói trang (default: 100)"
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=None,
        help="Gi·ªõi h·∫°n s·ªë trang (default: None = unlimited)"
    )
    parser.add_argument(
        "--update-existing",
        action="store_true",
        help="Update existing records (ch·ªâ cho full-etl mode)"
    )
    
    args = parser.parse_args()
    
    try:
        if args.mode == "fetch-all":
            result = fetch_newspaper_raw_all(
                page_size=args.page_size,
                max_pages=args.max_pages
            )
        elif args.mode == "fetch-only":
            result = fetch_newspaper_only(
                page_size=args.page_size,
                max_pages=args.max_pages
            )
        else:  # full-etl
            result = fetch_newspaper_full_etl(
                page_size=args.page_size,
                max_pages=args.max_pages,
                update_existing=args.update_existing
            )
        
        print_results(result)
        
        if result['status'] == 'success':
            sys.exit(0)
        else:
            sys.exit(1)
            
    except requests.exceptions.RequestException as e:
        print(f"\n‚ùå Error: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
