#!/usr/bin/env python3
"""
LLM Extract cho Lƒ©nh v·ª±c: X√ÇY D·ª∞NG ƒê·∫¢NG & H·ªÜ TH·ªêNG CH√çNH TR·ªä

THU·∫¶N LLM - Kh√¥ng d√πng Regex

Ngu·ªìn d·ªØ li·ªáu:
  - B·∫£ng: important_posts
  - Filter: type_newspaper = 'politics'
  - S·ªë l∆∞·ª£ng: ~3 posts

B·∫£ng ƒë√≠ch (3 b·∫£ng):
  1. cadre_statistics_detail       - Th·ªëng k√™ s·ªë l∆∞·ª£ng c√°n b·ªô/bi√™n ch·∫ø
  2. party_discipline_detail       - K·ª∑ lu·∫≠t ƒê·∫£ng/vi ph·∫°m
  3. cadre_quality_detail          - Ch·∫•t l∆∞·ª£ng c√°n b·ªô/ƒë√†o t·∫°o
"""

import os
import sys
import json
import time
import logging
import requests
from datetime import datetime
from typing import Dict, List, Optional, Any

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('call_llm/xay_dung_dang_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configuration
API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:7777")
LLM_API_KEY = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY", "")
LLM_MODEL = os.getenv("LLM_MODEL", "openai/gpt-4o-mini")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "10"))
DELAY_BETWEEN_CALLS = float(os.getenv("DELAY_BETWEEN_CALLS", "2"))  # seconds

if not LLM_API_KEY:
    logger.error("Kh√¥ng t√¨m th·∫•y API key")
    sys.exit(1)


def call_llm(prompt: str, max_retries: int = 3) -> Optional[str]:
    """Call OpenRouter LLM API"""
    url = "https://openrouter.ai/api/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {LLM_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": API_BASE_URL,
        "X-Title": "Xay Dung Dang Extractor"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 3000
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            logger.warning(f"LLM call attempt {attempt + 1}/{max_retries} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                logger.error(f"LLM call failed after {max_retries} attempts")
                return None


def extract_cadre_statistics(content: str, url: str, province: str) -> Optional[Dict]:
    """Extract th·ªëng k√™ s·ªë l∆∞·ª£ng c√°n b·ªô"""
    prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON theo ƒë√∫ng c·∫•u tr√∫c.
Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong vƒÉn b·∫£n, kh√¥ng suy di·ªÖn.

Schema:
{{
  "year": null,
  "quarter": null,
  "month": null,
  "total_authorized": null,
  "provincial_level": null,
  "commune_level": null,
  "contract_workers": null
}}

Gi·∫£i th√≠ch c√°c tr∆∞·ªùng:
- year (integer): NƒÉm c·ªßa b√°o c√°o
- quarter (integer 1-4): Qu√Ω (n·∫øu c√≥)
- month (integer 1-12): Th√°ng (n·∫øu c√≥)
- total_authorized (integer): T·ªïng s·ªë bi√™n ch·∫ø ƒë∆∞·ª£c giao/t·∫°m giao (ng∆∞·ªùi)
- provincial_level (integer): S·ªë bi√™n ch·∫ø c·∫•p t·ªânh/s·ªü ban ng√†nh (ng∆∞·ªùi)
- commune_level (integer): S·ªë bi√™n ch·∫ø c·∫•p x√£/ph∆∞·ªùng/th·ªã tr·∫•n (ng∆∞·ªùi)
- contract_workers (integer): S·ªë lao ƒë·ªông h·ª£p ƒë·ªìng (ng∆∞·ªùi)

Quy t·∫Øc:
1. QUAN TR·ªåNG: CH·ªà extract n·∫øu vƒÉn b·∫£n R√ï R√ÄNG n√≥i v·ªÅ H∆∞ng Y√™n (ƒê·∫£ng b·ªô t·ªânh H∆∞ng Y√™n ho·∫∑c c√°c huy·ªán/th√†nh ph·ªë thu·ªôc H∆∞ng Y√™n)
2. N·∫øu vƒÉn b·∫£n n√≥i v·ªÅ to√†n qu·ªëc, ƒë·∫£ng b·ªô t·ªânh kh√°c, ho·∫∑c kh√¥ng r√µ ƒë·ªãa ph∆∞∆°ng ‚Üí tr·∫£ v·ªÅ: {{"no_data": true}}
3. CH·ªà tr√≠ch xu·∫•t s·ªë li·ªáu C√ì TRONG vƒÉn b·∫£n v·ªÅ H∆∞ng Y√™n
4. N·∫øu vƒÉn b·∫£n KH√îNG ƒë·ªÅ c·∫≠p th·ªëng k√™ c√°n b·ªô/bi√™n ch·∫ø c·ªßa H∆∞ng Y√™n, tr·∫£ v·ªÅ: {{"no_data": true}}
5. C√°c s·ªë ph·∫£i l√† INTEGER (l√†m tr√≤n n·∫øu c·∫ßn)
6. N·∫øu tr∆∞·ªùng kh√¥ng c√≥ trong vƒÉn b·∫£n: ƒë·ªÉ null

T·ªânh/Th√†nh c·∫ßn validate: {province}

VƒÉn b·∫£n:
\"\"\"
{content[:3000]}
\"\"\"

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m gi·∫£i th√≠ch."""

    try:
        result = call_llm(prompt)
        if not result:
            return None
        
        # Parse JSON
        json_start = result.find("{")
        json_end = result.rfind("}") + 1
        if json_start == -1 or json_end == 0:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y JSON trong response")
            return None
        
        data = json.loads(result[json_start:json_end])
        
        if data.get("no_data"):
            logger.info(f"‚ÑπÔ∏è  Kh√¥ng c√≥ th√¥ng tin th·ªëng k√™ c√°n b·ªô")
            return None
        
        # Th√™m metadata
        data["province"] = province
        data["data_source"] = url
        
        return data
        
    except json.JSONDecodeError as e:
        logger.error(f"L·ªói parse JSON: {e}")
        return None
    except Exception as e:
        logger.error(f"L·ªói extract cadre statistics cho article {url}: {e}")
        return None


def extract_party_discipline(content: str, url: str, province: str) -> Optional[Dict]:
    """Extract th·ªëng k√™ k·ª∑ lu·∫≠t ƒê·∫£ng"""
    prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON theo ƒë√∫ng c·∫•u tr√∫c.
Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong vƒÉn b·∫£n, kh√¥ng suy di·ªÖn.

Schema:
{{
  "year": null,
  "quarter": null,
  "month": null,
  "dci_score": null,
  "discipline_violations": null,
  "warnings_issued": null,
  "dismissals": null,
  "compliance_rate": null,
  "regulation_adherence_score": null
}}

Gi·∫£i th√≠ch c√°c tr∆∞·ªùng:
- year (integer): NƒÉm c·ªßa b√°o c√°o
- quarter (integer 1-4): Qu√Ω (n·∫øu c√≥)
- month (integer 1-12): Th√°ng (n·∫øu c√≥)
- dci_score (float): ƒêi·ªÉm ch·ªâ s·ªë k·ª∑ lu·∫≠t ƒê·∫£ng DCI (0-100)
- discipline_violations (integer): S·ªë v·ª• vi ph·∫°m k·ª∑ lu·∫≠t ƒê·∫£ng
- warnings_issued (integer): S·ªë tr∆∞·ªùng h·ª£p b·ªã c·∫£nh c√°o
- dismissals (integer): S·ªë tr∆∞·ªùng h·ª£p b·ªã khai tr·ª´/c√°ch ch·ª©c
- compliance_rate (float): T·ª∑ l·ªá tu√¢n th·ªß k·ª∑ lu·∫≠t (%, 0-100)
- regulation_adherence_score (float): ƒêi·ªÉm ch·∫•p h√†nh n·ªôi quy (0-100)

Quy t·∫Øc:
1. QUAN TR·ªåNG: CH·ªà extract n·∫øu vƒÉn b·∫£n R√ï R√ÄNG n√≥i v·ªÅ H∆∞ng Y√™n (ƒê·∫£ng b·ªô t·ªânh H∆∞ng Y√™n ho·∫∑c c√°c huy·ªán/th√†nh ph·ªë thu·ªôc H∆∞ng Y√™n)
2. N·∫øu vƒÉn b·∫£n n√≥i v·ªÅ to√†n qu·ªëc, ƒë·∫£ng b·ªô t·ªânh kh√°c, ho·∫∑c kh√¥ng r√µ ƒë·ªãa ph∆∞∆°ng ‚Üí tr·∫£ v·ªÅ: {{"no_data": true}}
3. CH·ªà tr√≠ch xu·∫•t s·ªë li·ªáu C√ì TRONG vƒÉn b·∫£n v·ªÅ H∆∞ng Y√™n
4. N·∫øu vƒÉn b·∫£n KH√îNG ƒë·ªÅ c·∫≠p k·ª∑ lu·∫≠t ƒê·∫£ng/vi ph·∫°m/khai tr·ª´ c·ªßa H∆∞ng Y√™n, tr·∫£ v·ªÅ: {{"no_data": true}}
5. T·ª∑ l·ªá % chuy·ªÉn sang s·ªë th·∫≠p ph√¢n (98.5% ‚Üí 98.5)
6. N·∫øu tr∆∞·ªùng kh√¥ng c√≥ trong vƒÉn b·∫£n: ƒë·ªÉ null

T·ªânh/Th√†nh c·∫ßn validate: {province}

VƒÉn b·∫£n:
\"\"\"
{content[:3000]}
\"\"\"

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m gi·∫£i th√≠ch."""

    try:
        result = call_llm(prompt)
        if not result:
            return None
        
        json_start = result.find("{")
        json_end = result.rfind("}") + 1
        if json_start == -1 or json_end == 0:
            return None
        
        data = json.loads(result[json_start:json_end])
        
        if data.get("no_data"):
            logger.info(f"‚ÑπÔ∏è  URL {url} kh√¥ng c√≥ th√¥ng tin k·ª∑ lu·∫≠t ƒê·∫£ng")
            return None
        
        data["province"] = province
        data["url"] = url
        data["data_source"] = f"URL {url}"
        
        return data
        
    except Exception as e:
        logger.error(f"L·ªói extract party discipline cho article {url}: {e}")
        return None


def extract_cadre_quality(content: str, url: str, province: str) -> Optional[Dict]:
    """Extract ch·∫•t l∆∞·ª£ng c√°n b·ªô"""
    prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON theo ƒë√∫ng c·∫•u tr√∫c.
Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong vƒÉn b·∫£n, kh√¥ng suy di·ªÖn.

Schema:
{{
  "year": null,
  "quarter": null,
  "month": null,
  "total_cadres": null,
  "cadres_with_degree": null,
  "degree_rate": null,
  "training_completion_rate": null,
  "performance_score": null,
  "citizen_satisfaction": null,
  "policy_implementation_score": null
}}

Gi·∫£i th√≠ch c√°c tr∆∞·ªùng:
- year (integer): NƒÉm c·ªßa b√°o c√°o
- quarter (integer 1-4): Qu√Ω (n·∫øu c√≥)
- month (integer 1-12): Th√°ng (n·∫øu c√≥)
- total_cadres (integer): T·ªïng s·ªë c√°n b·ªô/c√¥ng ch·ª©c
- cadres_with_degree (integer): S·ªë c√°n b·ªô c√≥ b·∫±ng c·∫•p/tr√¨nh ƒë·ªô ƒë·∫°i h·ªçc tr·ªü l√™n
- degree_rate (float): T·ª∑ l·ªá c√°n b·ªô c√≥ b·∫±ng c·∫•p (%, 0-100)
- training_completion_rate (float): T·ª∑ l·ªá ho√†n th√†nh ƒë√†o t·∫°o/b·ªìi d∆∞·ª°ng (%, 0-100)
- performance_score (float): ƒêi·ªÉm ƒë√°nh gi√° hi·ªáu qu·∫£ c√¥ng t√°c (0-100)
- citizen_satisfaction (float): M·ª©c ƒë·ªô h√†i l√≤ng c·ªßa ng∆∞·ªùi d√¢n/doanh nghi·ªáp (%, 0-100)
- policy_implementation_score (float): ƒêi·ªÉm th·ª±c thi ch√≠nh s√°ch/nhi·ªám v·ª• (0-100)

Quy t·∫Øc:
1. QUAN TR·ªåNG: CH·ªà extract n·∫øu vƒÉn b·∫£n R√ï R√ÄNG n√≥i v·ªÅ H∆∞ng Y√™n (ƒê·∫£ng b·ªô t·ªânh H∆∞ng Y√™n ho·∫∑c c√°c huy·ªán/th√†nh ph·ªë thu·ªôc H∆∞ng Y√™n)
2. N·∫øu vƒÉn b·∫£n n√≥i v·ªÅ to√†n qu·ªëc, ƒë·∫£ng b·ªô t·ªânh kh√°c, ho·∫∑c kh√¥ng r√µ ƒë·ªãa ph∆∞∆°ng ‚Üí tr·∫£ v·ªÅ: {{"no_data": true}}
3. CH·ªà tr√≠ch xu·∫•t s·ªë li·ªáu C√ì TRONG vƒÉn b·∫£n v·ªÅ H∆∞ng Y√™n
4. N·∫øu vƒÉn b·∫£n KH√îNG ƒë·ªÅ c·∫≠p ch·∫•t l∆∞·ª£ng c√°n b·ªô/ƒë√†o t·∫°o/tr√¨nh ƒë·ªô c·ªßa H∆∞ng Y√™n, tr·∫£ v·ªÅ: {{"no_data": true}}
5. T·ª∑ l·ªá % chuy·ªÉn sang s·ªë th·∫≠p ph√¢n (90.5% ‚Üí 90.5)
6. N·∫øu tr∆∞·ªùng kh√¥ng c√≥ trong vƒÉn b·∫£n: ƒë·ªÉ null

T·ªânh/Th√†nh c·∫ßn validate: {province}

VƒÉn b·∫£n:
\"\"\"
{content[:3000]}
\"\"\"

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m gi·∫£i th√≠ch."""

    try:
        result = call_llm(prompt)
        if not result:
            return None
        
        json_start = result.find("{")
        json_end = result.rfind("}") + 1
        if json_start == -1 or json_end == 0:
            return None
        
        data = json.loads(result[json_start:json_end])
        
        if data.get("no_data"):
            logger.info(f"‚ÑπÔ∏è  URL {url} kh√¥ng c√≥ th√¥ng tin ch·∫•t l∆∞·ª£ng c√°n b·ªô")
            return None
        
        data["province"] = province
        data["url"] = url
        data["data_source"] = f"URL {url}"
        
        return data
        
    except Exception as e:
        logger.error(f"L·ªói extract cadre quality cho article {url}: {e}")
        return None


def save_to_detail_table(data: Dict, table_name: str) -> bool:
    """L∆∞u data v√†o b·∫£ng detail"""
    # Map url to economic_indicator_id if needed
    if "url" in data:
        data["economic_indicator_id"] = data.pop("url")
    
    endpoint = f"{API_BASE_URL}/api/indicators/{table_name}"
    
    try:
        response = requests.post(endpoint, json=data, timeout=30)
        
        if response.status_code in [200, 201]:
            logger.info(f"ƒê√£ l∆∞u v√†o {table_name}")
            return True
        elif response.status_code == 409 or "duplicate" in response.text.lower():
            logger.info(f"‚ÑπÔ∏è  {table_name} ƒë√£ t·ªìn t·∫°i (skip)")
            return True
        else:
            logger.error(f"L·ªói l∆∞u {table_name}: {response.status_code} - {response.text[:200]}")
            return False
            
    except Exception as e:
        logger.error(f"Exception khi l∆∞u {table_name}: {e}")
        return False


def get_politics_posts_from_db(limit: int = 100) -> List[Dict]:
    """L·∫•y important_posts c√≥ type_newspaper = 'politics' tr·ª±c ti·∫øp t·ª´ DB"""
    from sqlalchemy import create_engine, text
    from sqlalchemy.orm import sessionmaker
    import os
    
    try:
        # Get DB connection
        db_url = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/DBHuYe")
        engine = create_engine(db_url)
        Session = sessionmaker(bind=engine)
        session = Session()
        
        # Query important_posts
        query = text("""
            SELECT id, title, content, url, dvhc, published_date
            FROM important_posts
            WHERE type_newspaper = 'politics'
            ORDER BY id DESC
            LIMIT :limit
        """)
        
        result = session.execute(query, {"limit": limit})
        posts = []
        
        for row in result:
            posts.append({
                "id": row[0],
                "title": row[1],
                "content": row[2],
                "url": row[3],
                "province": row[4] or "H∆∞ng Y√™n",
                "published_date": row[5]
            })
        
        session.close()
        logger.info(f"T√¨m th·∫•y {len(posts)} posts t·ª´ important_posts (type_newspaper=politics)")
        return posts
        
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y posts t·ª´ DB: {e}")
        return []


def get_politics_articles(limit: int = 100) -> List[Dict]:
    """L·∫•y articles v·ªÅ ch√≠nh tr·ªã"""
    try:
        all_articles = []
        page = 1
        
        while len(all_articles) < limit:
            response = requests.get(
                f"{API_BASE_URL}/api/articles",
                params={
                    "page": page,
                    "page_size": min(100, limit - len(all_articles)),
                    "category": "politics"  # Filter by politics category
                },
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            articles = result.get("items", result.get("data", []))
            if not articles:
                break
                
            all_articles.extend(articles)
            
            if len(articles) < 100:  # No more pages
                break
            
            page += 1
        
        logger.info(f"T√¨m th·∫•y {len(all_articles)} articles v·ªÅ ch√≠nh tr·ªã")
        return all_articles[:limit]
        
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y articles: {e}")
        return []


def process_article(article: Dict) -> Dict[str, int]:
    """X·ª≠ l√Ω 1 article v√† extract c·∫£ 3 lo·∫°i th·ªëng k√™"""
    url = article.get("id")
    content = article.get("content", "")
    title = article.get("title", "")
    province = article.get("province", "H∆∞ng Y√™n")
    
    logger.info(f"\n{'='*60}")
    logger.info(f"URL ID: {url}")
    logger.info(f"Title: {title[:100]}")
    logger.info(f"Province: {province}")
    logger.info(f"Content length: {len(content)} chars")
    
    results = {
        "cadre_statistics": 0,
        "party_discipline": 0,
        "cadre_quality": 0
    }
    
    # 1. Extract cadre statistics
    logger.info("üíº Extracting cadre statistics...")
    cadre_stats = extract_cadre_statistics(content, url, province)
    if cadre_stats:
        if save_to_detail_table(cadre_stats, "cadre_statistics_detail"):
            results["cadre_statistics"] = 1
    time.sleep(DELAY_BETWEEN_CALLS)
    
    # 2. Extract party discipline
    logger.info("‚öñÔ∏è  Extracting party discipline...")
    party_disc = extract_party_discipline(content, url, province)
    if party_disc:
        if save_to_detail_table(party_disc, "party_discipline_detail"):
            results["party_discipline"] = 1
    time.sleep(DELAY_BETWEEN_CALLS)
    
    # 3. Extract cadre quality
    logger.info("‚≠ê Extracting cadre quality...")
    cadre_qual = extract_cadre_quality(content, url, province)
    if cadre_qual:
        if save_to_detail_table(cadre_qual, "cadre_quality_detail"):
            results["cadre_quality"] = 1
    time.sleep(DELAY_BETWEEN_CALLS)
    
    return results


def main():
    """Main function"""
    logger.info("="*80)
    logger.info("B·∫ÆT ƒê·∫¶U LLM EXTRACTION - Lƒ®NH V·ª∞C 1: X√ÇY D·ª∞NG ƒê·∫¢NG")
    logger.info(f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"LLM Model: {LLM_MODEL}")
    logger.info(f"üì¶ Batch size: {BATCH_SIZE}")
    logger.info("="*80)
    
    # L·∫•y posts t·ª´ important_posts (type_newspaper='politics')
    articles = get_politics_posts_from_db(limit=BATCH_SIZE)
    
    if not articles:
        logger.info("Kh√¥ng c√≥ articles n√†o c·∫ßn x·ª≠ l√Ω")
        return {
            "status": "no_data",
            "message": "Kh√¥ng c√≥ posts n√†o trong important_posts v·ªõi type_newspaper=politics",
            "processed": 0,
            "extracted": 0
        }
    
    # Process articles
    total_extracted = {
        "cadre_statistics": 0,
        "party_discipline": 0,
        "cadre_quality": 0
    }
    
    for i, article in enumerate(articles, 1):
        logger.info(f"\nProgress: {i}/{len(articles)}")
        
        try:
            results = process_article(article)
            
            for key, value in results.items():
                total_extracted[key] += value
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω article {article.get('id')}: {e}")
    
    # Summary
    logger.info("\n" + "="*80)
    logger.info("K·∫æT QU·∫¢ EXTRACTION")
    logger.info("="*80)
    logger.info(f"ƒê√£ x·ª≠ l√Ω: {len(articles)} articles")
    logger.info(f"üíº Cadre Statistics extracted: {total_extracted['cadre_statistics']}")
    logger.info(f"‚öñÔ∏è  Party Discipline extracted: {total_extracted['party_discipline']}")
    logger.info(f"‚≠ê Cadre Quality extracted: {total_extracted['cadre_quality']}")
    logger.info(f"T·ªïng: {sum(total_extracted.values())} records")
    logger.info("="*80)
    
    # Return results
    return {
        "status": "success",
        "message": "LLM extraction ho√†n th√†nh",
        "processed": len(articles),
        "extracted": total_extracted,
        "total_records": sum(total_extracted.values())
    }


if __name__ == "__main__":
    main()
