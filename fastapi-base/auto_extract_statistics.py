#!/usr/bin/env python3
"""
H·ªá th·ªëng AI t·ª± ƒë·ªông ƒë·ªçc t·ª´ng b√†i trong important_posts v√† extract statistics
Ch·∫°y t·ª± ƒë·ªông, kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng
"""

import os
import sys
import json
import time
import logging
import requests
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_extract_statistics.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configuration
API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:7777")
LLM_API_KEY = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY", "")
LLM_MODEL = os.getenv("LLM_MODEL", "openai/gpt-4-turbo")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "10"))  # S·ªë b√†i x·ª≠ l√Ω m·ªói l·∫ßn
DELAY_BETWEEN_CALLS = float(os.getenv("DELAY_BETWEEN_CALLS", "1"))  # seconds

if not LLM_API_KEY:
    logger.error("Kh√¥ng t√¨m th·∫•y OPENROUTER_API_KEY ho·∫∑c OPENAI_API_KEY trong environment")
    sys.exit(1)


def call_llm(prompt: str, max_retries: int = 3) -> Optional[str]:
    """Call OpenRouter LLM API with retry logic"""
    url = "https://openrouter.ai/api/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {LLM_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:7777",
        "X-Title": "Economic Statistics Extractor"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "temperature": 0.1,
        "max_tokens": 2000
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            logger.warning(f"LLM call attempt {attempt + 1}/{max_retries} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            else:
                logger.error(f"LLM call failed after {max_retries} attempts")
                return None


def get_unprocessed_posts() -> List[Dict]:
    """L·∫•y danh s√°ch c√°c b√†i ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω"""
    try:
        # L·∫•y t·∫•t c·∫£ important_posts (d√πng page v√† page_size)
        all_posts = []
        page = 1
        while True:
            response = requests.get(
                f"{API_BASE_URL}/api/important-posts",
                params={"page": page, "page_size": 100}
            )
            response.raise_for_status()
            result = response.json()
            
            # API tr·∫£ v·ªÅ dict v·ªõi key 'items' ho·∫∑c 'data'
            if isinstance(result, dict):
                posts = result.get("items", result.get("data", []))
                all_posts.extend(posts)
                
                # Check n·∫øu h·∫øt data
                total = result.get("total", 0)
                if len(all_posts) >= total or len(posts) == 0:
                    break
            else:
                all_posts.extend(result)
                break
            
            page += 1
        
        # L·∫•y danh s√°ch post_id ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        processed_economic = set()
        processed_political = set()
        
        # Check economic_statistics
        econ_response = requests.get(
            f"{API_BASE_URL}/api/statistics/economic",
            params={"page": 1, "page_size": 1000}
        )
        if econ_response.status_code == 200:
            econ_data = econ_response.json()
            econ_records = econ_data if isinstance(econ_data, list) else econ_data.get("items", econ_data.get("data", []))
            for record in econ_records:
                if record.get("source_post_id"):
                    processed_economic.add(record["source_post_id"])
        
        # Check political_statistics
        pol_response = requests.get(
            f"{API_BASE_URL}/api/statistics/political",
            params={"page": 1, "page_size": 1000}
        )
        if pol_response.status_code == 200:
            pol_data = pol_response.json()
            pol_records = pol_data if isinstance(pol_data, list) else pol_data.get("items", pol_data.get("data", []))
            for record in pol_records:
                if record.get("source_post_id"):
                    processed_political.add(record["source_post_id"])
        
        # Filter ch·ªâ l·∫•y b√†i Th∆∞ V≈© v√† Tr√† L√Ω ch∆∞a x·ª≠ l√Ω
        TARGET_LOCATIONS = ["Th∆∞ V≈©", "Tr√† L√Ω", "Thu Vu", "Tra Ly"]
        unprocessed = []
        for post in all_posts:
            post_id = post.get("id")
            dvhc = post.get("dvhc", "")
            
            # Ki·ªÉm tra c√≥ ph·∫£i Th∆∞ V≈© ho·∫∑c Tr√† L√Ω kh√¥ng
            is_target_location = any(loc.lower() in str(dvhc).lower() for loc in TARGET_LOCATIONS)
            
            # B√†i ch∆∞a x·ª≠ l√Ω = l√† ƒë·ªãa ph∆∞∆°ng m·ª•c ti√™u V√Ä (ch∆∞a c√≥ trong c·∫£ 2 b·∫£ng HO·∫∂C ƒë√£ x√≥a h·∫øt)
            if is_target_location:
                if post_id not in processed_economic and post_id not in processed_political:
                    unprocessed.append(post)
        
        logger.info(f"T·ªïng s·ªë b√†i: {len(all_posts)}, ƒê√£ x·ª≠ l√Ω: {len(processed_economic | processed_political)}, Th∆∞ V≈©/Tr√† L√Ω ch∆∞a x·ª≠ l√Ω: {len(unprocessed)}")
        return unprocessed
        
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y danh s√°ch b√†i: {e}")
        return []


def extract_economic_data(content: str, dvhc: str, source_url: str, post_id: int) -> Optional[Dict]:
    """Extract economic statistics t·ª´ content"""
    prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr√≠ch xu·∫•t th√¥ng tin kinh t·∫ø (n·∫øu c√≥):

VƒÉn b·∫£n: {content[:3000]}

ƒê·ªãa ph∆∞∆°ng: {dvhc}

Tr√≠ch xu·∫•t c√°c th√¥ng tin sau (n·∫øu kh√¥ng c√≥ th√¨ ƒë·ªÉ null):
- year: NƒÉm (integer)
- period: Giai ƒëo·∫°n/K·ª≥ (string, v√≠ d·ª•: "Qu√Ω I", "6 th√°ng ƒë·∫ßu nƒÉm", "5 nƒÉm 2020-2025")
- total_production_value: T·ªïng gi√° tr·ªã s·∫£n xu·∫•t (float, ƒë∆°n v·ªã t·ª∑ ƒë·ªìng)
- growth_rate: T·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng (float, v√≠ d·ª•: 8.5 nghƒ©a l√† 8.5%)
- total_budget_revenue: T·ªïng thu ng√¢n s√°ch (float, ƒë∆°n v·ªã t·ª∑ ƒë·ªìng)
- budget_collection_efficiency: Hi·ªáu su·∫•t thu ng√¢n s√°ch (float, v√≠ d·ª•: 120.5 nghƒ©a l√† 120.5%)

Tr·∫£ v·ªÅ JSON format:
{{
    "year": 2025,
    "period": "6 th√°ng ƒë·∫ßu nƒÉm",
    "total_production_value": 1500.5,
    "growth_rate": 8.5,
    "total_budget_revenue": 200.0,
    "budget_collection_efficiency": 120.5
}}

N·∫øu vƒÉn b·∫£n KH√îNG c√≥ th√¥ng tin kinh t·∫ø, tr·∫£ v·ªÅ: {{"no_data": true}}
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m."""

    try:
        result = call_llm(prompt)
        if not result:
            return None
        
        # Parse JSON t·ª´ response
        json_start = result.find("{")
        json_end = result.rfind("}") + 1
        if json_start == -1 or json_end == 0:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y JSON trong response cho post {post_id}")
            return None
        
        data = json.loads(result[json_start:json_end])
        
        # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu kinh t·∫ø
        if data.get("no_data"):
            logger.info(f"‚ÑπÔ∏è  Post {post_id} kh√¥ng c√≥ th√¥ng tin kinh t·∫ø")
            return None
        
        # Th√™m metadata
        data["dvhc"] = dvhc if dvhc else "Unknown"
        data["source_post_id"] = post_id
        data["source_url"] = source_url
        
        return data
        
    except json.JSONDecodeError as e:
        logger.error(f"L·ªói parse JSON cho post {post_id}: {e}")
        return None
    except Exception as e:
        logger.error(f"L·ªói extract economic data cho post {post_id}: {e}")
        return None


def extract_political_data(content: str, dvhc: str, source_url: str, post_id: int) -> Optional[Dict]:
    """Extract political statistics t·ª´ content"""
    prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr√≠ch xu·∫•t th√¥ng tin ch√≠nh tr·ªã/ƒê·∫£ng (n·∫øu c√≥):

VƒÉn b·∫£n: {content[:3000]}

ƒê·ªãa ph∆∞∆°ng: {dvhc}

Tr√≠ch xu·∫•t c√°c th√¥ng tin sau (n·∫øu kh√¥ng c√≥ th√¨ ƒë·ªÉ null):
- year: NƒÉm (integer)
- period: Giai ƒëo·∫°n (string)
- party_organization_count: S·ªë t·ªï ch·ª©c ƒê·∫£ng (integer)
- party_member_count: S·ªë ƒê·∫£ng vi√™n (integer)
- party_size_description: M√¥ t·∫£ quy m√¥ ƒê·∫£ng (string)
- new_party_members: S·ªë ƒê·∫£ng vi√™n m·ªõi (integer)
- party_cells_count: S·ªë chi b·ªô (integer)

Tr·∫£ v·ªÅ JSON format:
{{
    "year": 2025,
    "period": "Qu√Ω III",
    "party_organization_count": 50,
    "party_member_count": 1500,
    "party_size_description": "ƒê·∫£ng b·ªô c√≥ 50 t·ªï ch·ª©c...",
    "new_party_members": 70,
    "party_cells_count": 95
}}

N·∫øu vƒÉn b·∫£n KH√îNG c√≥ th√¥ng tin ch√≠nh tr·ªã/ƒê·∫£ng, tr·∫£ v·ªÅ: {{"no_data": true}}
Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m."""

    try:
        result = call_llm(prompt)
        if not result:
            return None
        
        # Parse JSON t·ª´ response
        json_start = result.find("{")
        json_end = result.rfind("}") + 1
        if json_start == -1 or json_end == 0:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y JSON trong response cho post {post_id}")
            return None
        
        data = json.loads(result[json_start:json_end])
        
        # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ch√≠nh tr·ªã
        if data.get("no_data"):
            logger.info(f"‚ÑπÔ∏è  Post {post_id} kh√¥ng c√≥ th√¥ng tin ch√≠nh tr·ªã")
            return None
        
        # Th√™m metadata
        data["dvhc"] = dvhc if dvhc else "Unknown"
        data["source_post_id"] = post_id
        data["source_url"] = source_url
        
        return data
        
    except json.JSONDecodeError as e:
        logger.error(f"L·ªói parse JSON cho post {post_id}: {e}")
        return None
    except Exception as e:
        logger.error(f"L·ªói extract political data cho post {post_id}: {e}")
        return None


def save_to_database(data: Dict, data_type: str) -> bool:
    """L∆∞u data v√†o database qua API"""
    endpoint = f"{API_BASE_URL}/api/statistics/{data_type}"
    
    try:
        response = requests.post(endpoint, json=data)
        
        if response.status_code in [200, 201]:
            logger.info(f"ƒê√£ l∆∞u {data_type} cho post {data.get('source_post_id')}")
            return True
        elif response.status_code == 409 or "duplicate" in response.text.lower():
            logger.info(f"‚ÑπÔ∏è  {data_type} cho post {data.get('source_post_id')} ƒë√£ t·ªìn t·∫°i (skip)")
            return True
        else:
            logger.error(f"L·ªói l∆∞u {data_type}: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        logger.error(f"Exception khi l∆∞u {data_type}: {e}")
        return False


def process_post(post: Dict) -> Tuple[bool, bool]:
    """
    X·ª≠ l√Ω 1 b√†i post
    Returns: (economic_extracted, political_extracted)
    """
    post_id = post.get("id")
    content = post.get("content", "")
    dvhc = post.get("dvhc", "")
    source_url = post.get("source_url", "")
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ƒêang x·ª≠ l√Ω post ID: {post_id}")
    logger.info(f"ƒêVHC: {dvhc}")
    logger.info(f"Content length: {len(content)} chars")
    
    economic_saved = False
    political_saved = False
    
    # Extract economic data
    logger.info("üí∞ ƒêang extract economic data...")
    economic_data = extract_economic_data(content, dvhc, source_url, post_id)
    if economic_data:
        economic_saved = save_to_database(economic_data, "economic")
        time.sleep(DELAY_BETWEEN_CALLS)
    
    # Extract political data
    logger.info("üèõÔ∏è  ƒêang extract political data...")
    political_data = extract_political_data(content, dvhc, source_url, post_id)
    if political_data:
        political_saved = save_to_database(political_data, "political")
        time.sleep(DELAY_BETWEEN_CALLS)
    
    return economic_saved, political_saved


def main():
    """Main function - ch·∫°y auto extract"""
    logger.info("="*80)
    logger.info("B·∫ÆT ƒê·∫¶U AUTO EXTRACT STATISTICS")
    logger.info(f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"LLM Model: {LLM_MODEL}")
    logger.info(f"üì¶ Batch size: {BATCH_SIZE}")
    logger.info("="*80)
    
    # L·∫•y danh s√°ch b√†i ch∆∞a x·ª≠ l√Ω
    unprocessed_posts = get_unprocessed_posts()
    
    if not unprocessed_posts:
        logger.info("Kh√¥ng c√≥ b√†i n√†o c·∫ßn x·ª≠ l√Ω. T·∫•t c·∫£ ƒë√£ ƒë∆∞·ª£c extract!")
        return
    
    # Process theo batch
    total_posts = len(unprocessed_posts)
    economic_count = 0
    political_count = 0
    error_count = 0
    
    for i, post in enumerate(unprocessed_posts[:BATCH_SIZE], 1):
        logger.info(f"\nProgress: {i}/{min(BATCH_SIZE, total_posts)}")
        
        try:
            economic_ok, political_ok = process_post(post)
            
            if economic_ok:
                economic_count += 1
            if political_ok:
                political_count += 1
            if not economic_ok and not political_ok:
                error_count += 1
                
        except Exception as e:
            logger.error(f"L·ªói khi x·ª≠ l√Ω post {post.get('id')}: {e}")
            error_count += 1
        
        # Delay gi·ªØa c√°c b√†i
        if i < min(BATCH_SIZE, total_posts):
            time.sleep(DELAY_BETWEEN_CALLS)
    
    # Summary
    logger.info("\n" + "="*80)
    logger.info("K·∫æT QU·∫¢ AUTO EXTRACT")
    logger.info("="*80)
    logger.info(f"ƒê√£ x·ª≠ l√Ω: {min(BATCH_SIZE, total_posts)} b√†i")
    logger.info(f"üí∞ Economic records extracted: {economic_count}")
    logger.info(f"üèõÔ∏è  Political records extracted: {political_count}")
    logger.info(f"Errors: {error_count}")
    logger.info(f"üìã C√≤n l·∫°i ch∆∞a x·ª≠ l√Ω: {max(0, total_posts - BATCH_SIZE)} b√†i")
    logger.info("="*80)
    
    if total_posts > BATCH_SIZE:
        logger.info(f"\nTIP: Ch·∫°y l·∫°i script ƒë·ªÉ x·ª≠ l√Ω ti·∫øp {total_posts - BATCH_SIZE} b√†i c√≤n l·∫°i")


if __name__ == "__main__":
    main()
