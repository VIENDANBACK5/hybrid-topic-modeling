# Data Pipeline Guide

## üìä Data Flow Architecture

```
External API / Database
        ‚Üì
   data/raw/          ‚Üê Raw data (JSON)
        ‚Üì
  üîß Processing       ‚Üê Clean, normalize, validate
        ‚Üì
  data/processed/    ‚Üê Processed data (JSON)
        ‚Üì
   Database          ‚Üê Structured data
        ‚Üì
   ü§ñ Training       ‚Üê BERTopic, Classification
        ‚Üì
  data/results/      ‚Üê Training results
```

## üéØ Quy tr√¨nh chu·∫©n

### 1Ô∏è‚É£ **Fetch Data t·ª´ BE kh√°c**

```bash
# Fetch t·ª´ external API
curl -X POST http://localhost:7777/api/data/fetch-external \
  -H "Content-Type: application/json" \
  -d '{
    "api_url": "http://192.168.30.28:8000/api/articles",
    "params": {"limit": 500}
  }'

# K·∫øt qu·∫£: data/raw/raw_20260104_120000.json
```

### 2Ô∏è‚É£ **X·ª≠ l√Ω Raw Data**

```bash
# Process: clean, normalize, validate
curl -X POST http://localhost:7777/api/data/process \
  -H "Content-Type: application/json" \
  -d '{
    "raw_file": "data/raw/raw_20260104_120000.json"
  }'

# K·∫øt qu·∫£: data/processed/processed_20260104_120000.json
```

**Processing actions:**
- ‚úÖ Normalize data structure
- ‚úÖ Clean HTML tags, special characters
- ‚úÖ Extract metadata (source, category, url)
- ‚úÖ Validate minimum content length
- ‚úÖ Filter invalid records

### 3Ô∏è‚É£ **Load v√†o Database**

```bash
# Load processed data ‚Üí database
curl -X POST http://localhost:7777/api/data/load-to-db \
  -H "Content-Type: application/json" \
  -d '{
    "processed_file": "data/processed/processed_20260104_120000.json",
    "update_existing": false
  }'
```

### 4Ô∏è‚É£ **Train t·ª´ Processed File**

```bash
# Train BERTopic t·ª´ processed file (kh√¥ng c·∫ßn database)
curl -X POST http://localhost:7777/api/topics/train \
  -H "Content-Type: application/json" \
  -d '{
    "from_processed_file": "data/processed/processed_20260104_120000.json",
    "min_topic_size": 10
  }'
```

## ‚ö° ONE-COMMAND ETL

```bash
# Full ETL: Fetch ‚Üí Process ‚Üí Load ‚Üí Database
curl -X POST "http://localhost:7777/api/data/full-etl?use_database=true&limit=500"

# Ho·∫∑c t·ª´ external API
curl -X POST "http://localhost:7777/api/data/full-etl?external_api_url=http://192.168.30.28:8000/api/articles"
```

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c

```
data/
‚îú‚îÄ‚îÄ raw/                    # Raw data t·ª´ API
‚îÇ   ‚îú‚îÄ‚îÄ raw_20260104_120000.json
‚îÇ   ‚îú‚îÄ‚îÄ raw_20260104_130000.json
‚îÇ   ‚îî‚îÄ‚îÄ raw_from_db_20260104.json
‚îÇ
‚îú‚îÄ‚îÄ processed/              # Data ƒë√£ x·ª≠ l√Ω
‚îÇ   ‚îú‚îÄ‚îÄ processed_20260104_120000.json
‚îÇ   ‚îú‚îÄ‚îÄ processed_20260104_130000.json
‚îÇ   ‚îî‚îÄ‚îÄ processed_clean_500.json
‚îÇ
‚îú‚îÄ‚îÄ results/                # Training results
‚îÇ   ‚îú‚îÄ‚îÄ bertopic_session_abc123/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topics.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizations/
‚îÇ   ‚îî‚îÄ‚îÄ training_logs/
‚îÇ
‚îú‚îÄ‚îÄ models/                 # Saved models
‚îÇ   ‚îî‚îÄ‚îÄ bertopic_latest.pkl
‚îÇ
‚îî‚îÄ‚îÄ cache/                  # Cache data
    ‚îî‚îÄ‚îÄ topicgpt/
```

## üîÑ Workflows th∆∞·ªùng d√πng

### Workflow 1: Sync t·ª´ External API h√†ng ng√†y

```bash
#!/bin/bash
# daily_sync.sh

# 1. Fetch data
curl -X POST http://localhost:7777/api/data/fetch-external \
  -d '{"api_url": "http://external-api.com/articles"}' \
  > fetch_result.json

RAW_FILE=$(jq -r '.result.raw_file' fetch_result.json)

# 2. Process
curl -X POST http://localhost:7777/api/data/process \
  -d "{\"raw_file\": \"$RAW_FILE\"}" \
  > process_result.json

PROCESSED_FILE=$(jq -r '.result.processed_file' process_result.json)

# 3. Load to DB
curl -X POST http://localhost:7777/api/data/load-to-db \
  -d "{\"processed_file\": \"$PROCESSED_FILE\"}"

# 4. Run full pipeline (classify, sentiment, etc.)
curl -X POST http://localhost:7777/api/orchestrator/quick-update?limit=500
```

### Workflow 2: Train t·ª´ Processed File

```bash
# C√≥ s·∫µn processed file, kh√¥ng c·∫ßn database
curl -X POST http://localhost:7777/api/topics/train \
  -H "Content-Type: application/json" \
  -d '{
    "from_processed_file": "data/processed/processed_clean_1000.json",
    "min_topic_size": 15,
    "enable_topicgpt": true
  }'
```

### Workflow 3: Export DB ‚Üí Process ‚Üí Train

```bash
# 1. Export database to raw
curl -X POST "http://localhost:7777/api/data/export-db-to-raw?limit=1000" \
  > export_result.json

RAW_FILE=$(jq -r '.result.raw_file' export_result.json)

# 2. Process
curl -X POST http://localhost:7777/api/data/process \
  -d "{\"raw_file\": \"$RAW_FILE\"}" \
  > process_result.json

PROCESSED_FILE=$(jq -r '.result.processed_file' process_result.json)

# 3. Train t·ª´ processed file
curl -X POST http://localhost:7777/api/topics/train \
  -d "{\"from_processed_file\": \"$PROCESSED_FILE\"}"
```

## üìã Qu·∫£n l√Ω Files

### List files

```bash
# Xem raw files
curl http://localhost:7777/api/data/files/raw

# Xem processed files
curl http://localhost:7777/api/data/files/processed
```

### File naming convention

```
raw_YYYYMMDD_HHMMSS.json          # Raw data v·ªõi timestamp
raw_from_db_YYYYMMDD.json         # Export t·ª´ database
processed_YYYYMMDD_HHMMSS.json    # Processed data
processed_clean_1000.json         # Custom name v·ªõi size
```

## üéØ Best Practices

### 1. L∆∞u tr·ªØ organized

- ‚úÖ Raw data: L∆∞u nguy√™n t·ª´ API (traceability)
- ‚úÖ Processed data: Clean, validate (ready for training)
- ‚úÖ Version by timestamp (d·ªÖ rollback)

### 2. Processing pipeline

```python
from app.services.etl.data_pipeline import get_data_pipeline

pipeline = get_data_pipeline(db)

# Step 1: Fetch
fetch_result = pipeline.fetch_and_save_raw_data(
    external_api_url="http://api.com/data"
)

# Step 2: Process
process_result = pipeline.process_raw_data(
    raw_file=fetch_result["raw_file"]
)

# Step 3: Load
load_result = pipeline.load_processed_data_to_db(
    processed_file=process_result["processed_file"]
)

# Step 4: Train t·ª´ processed file
from app.services.topic.bertopic_trainer import get_trainer
trainer = get_trainer(db)
train_result = trainer.train_from_articles(
    from_processed_file=process_result["processed_file"],
    min_topic_size=10
)
```

### 3. Incremental updates

```bash
# Ch·ªâ x·ª≠ l√Ω data m·ªõi
curl -X POST http://localhost:7777/api/data/full-etl?limit=100

# Quick update pipeline
curl -X POST http://localhost:7777/api/orchestrator/quick-update?limit=100
```

## ‚öôÔ∏è Configuration

### Environment variables

```bash
# Data directories
export DATA_DIR=data
export RAW_DIR=data/raw
export PROCESSED_DIR=data/processed

# External API
export EXTERNAL_API_URL=http://192.168.30.28:8000
export EXTERNAL_API_TOKEN=your_token
```

### Processing options

```python
# Customize cleaner
from app.services.etl.text_cleaner import TextCleaner

cleaner = TextCleaner(
    remove_html=True,
    remove_urls=True,
    remove_emails=True,
    lowercase=False  # Keep original case for Vietnamese
)
```

## üîç Monitoring

### Check pipeline status

```bash
# List recent files
curl http://localhost:7777/api/data/files/processed | jq '.files[0:5]'

# Check training sessions
curl http://localhost:7777/api/topics/sessions

# System status
curl http://localhost:7777/api/orchestrator/status
```

## üö® Troubleshooting

### Problem: External API timeout

```bash
# Solution: Fetch v·ªõi limit nh·ªè h∆°n
curl -X POST .../fetch-external -d '{"params": {"limit": 100}}'
```

### Problem: Processing fails

```bash
# Check raw file format
cat data/raw/raw_file.json | jq '.[0]'

# Verify data structure
curl -X POST .../process -d '{"raw_file": "..."}'
```

### Problem: Training out of memory

```bash
# Train t·ª´ processed file v·ªõi limit
curl -X POST .../train -d '{
  "from_processed_file": "...",
  "limit": 500
}'
```

## üìö API Reference

See: http://localhost:7777/docs
- Section: **üìä Data Pipeline**
- Endpoints: `/api/data/*`

---

**Quy tr√¨nh chu·∫©n: External API ‚Üí Raw ‚Üí Processed ‚Üí DB ‚Üí Training**
