"""
Full Pipeline API - Orchestrated pipeline endpoints
Provides automatic full-flow: Crawl ‚Üí ETL ‚Üí NER ‚Üí Topic ‚Üí Index ‚Üí Dashboard
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional, Dict
from datetime import datetime
import logging

from app.services.task_queue import get_task_queue, init_task_handlers, TaskStatus
from app.services.monitoring import get_metrics_collector, get_health_checker

logger = logging.getLogger(__name__)
router = APIRouter()

# Initialize task handlers
try:
    init_task_handlers()
except Exception as e:
    logger.warning(f"Task handlers init: {e}")


# ============================================
# Request/Response Models
# ============================================

class FullPipelineRequest(BaseModel):
    """Request for full pipeline execution"""
    url: str
    mode: str = "max"  # quick, max, full
    train_topics: bool = True
    extract_ner: bool = True
    save_to_db: bool = True
    async_mode: bool = True  # Run in background


class FullPipelineResponse(BaseModel):
    """Response for full pipeline"""
    status: str
    task_id: Optional[str] = None
    message: str
    result: Optional[Dict] = None


class TaskStatusResponse(BaseModel):
    """Task status response"""
    id: str
    name: str
    status: str
    progress: int
    created_at: str
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    result: Optional[Dict] = None
    error: Optional[str] = None


# ============================================
# Full Pipeline Endpoints
# ============================================

@router.post("/run", response_model=FullPipelineResponse)
async def run_full_pipeline(request: FullPipelineRequest):
    """
    üöÄ FULL PIPELINE - Ch·∫°y to√†n b·ªô lu·ªìng t·ª± ƒë·ªông
    
    Lu·ªìng: Crawl ‚Üí Clean ‚Üí Dedupe ‚Üí NER ‚Üí Topic Modeling ‚Üí Save DB
    
    Args:
        url: URL website c·∫ßn crawl
        mode: "quick" (50 trang), "max" (5000 trang), "full" (10000 trang)
        train_topics: T·ª± ƒë·ªông train topic model
        extract_ner: Tr√≠ch xu·∫•t Named Entities
        save_to_db: L∆∞u v√†o database
        async_mode: Ch·∫°y background (khuy·∫øn ngh·ªã cho crawl l·ªõn)
    
    Returns:
        task_id: ID ƒë·ªÉ theo d√µi ti·∫øn ƒë·ªô (n·∫øu async)
        result: K·∫øt qu·∫£ (n·∫øu sync)
    """
    try:
        queue = get_task_queue()
        metrics = get_metrics_collector()
        
        # Record pipeline start
        metrics.record_pipeline_event('crawl', 1)
        
        params = {
            'url': request.url,
            'mode': request.mode,
            'train_topics': request.train_topics,
            'extract_ner': request.extract_ner,
            'save_to_db': request.save_to_db,
        }
        
        if request.async_mode:
            # Run in background
            task_id = await queue.submit('full_pipeline', params)
            
            return FullPipelineResponse(
                status="submitted",
                task_id=task_id,
                message=f"Pipeline ƒë√£ ƒë∆∞·ª£c submit. Theo d√µi t·∫°i /api/orchestrator/task/{task_id}"
            )
        else:
            # Run synchronously (for small crawls)
            from app.services.task_queue import handle_full_pipeline
            
            result = await handle_full_pipeline(params, lambda p, m: None)
            
            # Record success
            metrics.record_pipeline_event('crawl', success=True)
            if result.get('stages', {}).get('topics', {}).get('num_topics'):
                metrics.record_pipeline_event('topics', result['stages']['topics']['num_topics'])
            
            return FullPipelineResponse(
                status="completed",
                message="Pipeline ho√†n th√†nh",
                result=result
            )
    
    except Exception as e:
        logger.error(f"Pipeline error: {e}", exc_info=True)
        metrics = get_metrics_collector()
        metrics.record_pipeline_event('crawl', success=False)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/task/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """
    üìä L·∫•y tr·∫°ng th√°i task
    
    Args:
        task_id: ID c·ªßa task c·∫ßn ki·ªÉm tra
    
    Returns:
        Task status v·ªõi progress v√† result
    """
    queue = get_task_queue()
    task = queue.get_task(task_id)
    
    if not task:
        raise HTTPException(status_code=404, detail=f"Task {task_id} kh√¥ng t·ªìn t·∫°i")
    
    return TaskStatusResponse(**task)


@router.get("/tasks", response_model=List[TaskStatusResponse])
async def list_tasks(limit: int = 20):
    """
    üìã Li·ªát k√™ c√°c tasks g·∫ßn ƒë√¢y
    
    Args:
        limit: S·ªë l∆∞·ª£ng tasks t·ªëi ƒëa
    """
    queue = get_task_queue()
    tasks = queue.get_all_tasks(limit=limit)
    return [TaskStatusResponse(**t) for t in tasks]


@router.post("/task/{task_id}/cancel")
async def cancel_task(task_id: str):
    """
    ‚ùå H·ªßy task ƒëang ch·∫°y
    """
    queue = get_task_queue()
    success = await queue.cancel_task(task_id)
    
    if success:
        return {"status": "cancelled", "task_id": task_id}
    else:
        raise HTTPException(status_code=400, detail="Kh√¥ng th·ªÉ h·ªßy task")


# ============================================
# Monitoring Endpoints
# ============================================

@router.get("/metrics")
async def get_metrics():
    """
    üìà L·∫•y metrics h·ªá th·ªëng
    
    Returns:
        - System metrics (CPU, RAM, Disk)
        - Request stats (latency, errors)
        - Pipeline stats (crawls, topics, etc.)
    """
    metrics = get_metrics_collector()
    return metrics.get_all_metrics()


@router.get("/metrics/system")
async def get_system_metrics():
    """Get system metrics only"""
    metrics = get_metrics_collector()
    return metrics.get_system_metrics()


@router.get("/metrics/pipeline")
async def get_pipeline_metrics():
    """Get pipeline metrics only"""
    metrics = get_metrics_collector()
    return metrics.get_pipeline_stats()


@router.get("/health")
async def health_check():
    """
    üè• Health check endpoint
    
    Checks:
        - Database connectivity
        - Redis connectivity
        - Disk space
        - Memory usage
    """
    checker = get_health_checker()
    result = checker.check_all()
    
    status_code = 200
    if result['status'] == 'unhealthy':
        status_code = 503
    elif result['status'] == 'degraded':
        status_code = 200  # Still OK but with warning
    
    return result


@router.get("/health/live")
async def liveness_probe():
    """Kubernetes liveness probe"""
    return {"status": "alive", "timestamp": datetime.now().isoformat()}


@router.get("/health/ready")
async def readiness_probe():
    """Kubernetes readiness probe"""
    checker = get_health_checker()
    result = checker.check_all()
    
    if result['status'] == 'unhealthy':
        raise HTTPException(status_code=503, detail="Service not ready")
    
    return {"status": "ready", "timestamp": datetime.now().isoformat()}


# ============================================
# Quick Pipeline Shortcuts
# ============================================

@router.post("/quick-crawl")
async def quick_crawl(url: str, max_pages: int = 50):
    """
    ‚ö° Quick crawl - Crawl nhanh kh√¥ng train topic
    """
    return await run_full_pipeline(FullPipelineRequest(
        url=url,
        mode="quick",
        train_topics=False,
        extract_ner=True,
        async_mode=False
    ))


@router.post("/full-analysis")
async def full_analysis(url: str):
    """
    üî¨ Full analysis - Crawl + NER + Topics (async)
    """
    return await run_full_pipeline(FullPipelineRequest(
        url=url,
        mode="max",
        train_topics=True,
        extract_ner=True,
        async_mode=True
    ))
