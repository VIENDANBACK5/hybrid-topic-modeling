"""
Vietnamese Tokenizer cho BERTopic
S·ª≠ d·ª•ng underthesea - th∆∞ vi·ªán NLP ti·∫øng Vi·ªát ph·ªï bi·∫øn nh·∫•t
T√≠ch h·ª£p TextCleaner cho preprocessing chuy√™n s√¢u
"""
import re
import logging
from typing import List, Callable, Optional

logger = logging.getLogger(__name__)

# Initialize TextCleaner singleton
_text_cleaner = None
_text_cleaner_initialized = False

def get_text_cleaner():
    """Get or create TextCleaner singleton"""
    global _text_cleaner, _text_cleaner_initialized

    if _text_cleaner_initialized:
        return _text_cleaner
    
    _text_cleaner_initialized = True

    if _text_cleaner is None:
        try:
            from app.services.etl.text_cleaner import TextCleaner
            _text_cleaner = TextCleaner()
            logger.info("‚úÖ TextCleaner initialized for preprocessing")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not initialize TextCleaner: {e}")
            _text_cleaner = None
    return _text_cleaner

# Vietnamese stopwords
VIETNAMESE_STOPWORDS = {
    'v√†', 'c·ªßa', 'cho', 'v·ªõi', 't·ª´', 'trong', 'tr√™n', 'd∆∞·ªõi', 'sau', 'tr∆∞·ªõc',
    'khi', 'n·∫øu', 'th√¨', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'b·ªã', 's·∫Ω', 'ƒë√£', 'ƒëang',
    'm√†', 'nh∆∞ng', 'ho·∫∑c', 'n√™n', 'ƒë·ªÉ', 'v√¨', 'do', 'b·ªüi', 'theo', 'v·ªÅ',
    'n√†y', 'ƒë√≥', 'kia', 'ƒë√¢y', 'ƒë√¢u', 'n√†o', 'sao', 'th·∫ø', 'v·∫≠y', 'th·∫ø n√†o',
    't√¥i', 'b·∫°n', 'anh', 'ch·ªã', 'em', 'ch√∫ng', 'h·ªç', 'm√¨nh', 'ta', 'ng∆∞·ªùi',
    'c√°i', 'con', 'chi·∫øc', 'b√†i', 'vi·ªác', 'ƒëi·ªÅu', 'n∆°i', 'l√∫c', 'th·ªùi',
    'r·∫•t', 'qu√°', 'l·∫Øm', 'nh·∫•t', 'h∆°n', 'k√©m', 'b·∫±ng', 'v·ªõi', 'nh∆∞',
    'm·ªôt', 'hai', 'ba', 'b·ªën', 'nƒÉm', 's√°u', 'b·∫£y', 't√°m', 'ch√≠n', 'm∆∞·ªùi',
    'nhi·ªÅu', '√≠t', 't·∫•t c·∫£', 'm·ªói', 'm·ªçi', 'c√°c', 'nh·ªØng', 'm·∫•y',
    'kh√¥ng', 'ch∆∞a', 'ch·∫≥ng', 'ch·∫£', 'ƒë·ª´ng', 'kh√¥ng ph·∫£i',
    'th√¨', 'l√†', '·ªü', 't·∫°i', 'v√†o', 'ra', 'l√™n', 'xu·ªëng', 'qua', 'l·∫°i',
    'c≈©ng', 'c√πng', 'v·∫´n', 'ƒë·ªÅu', 'm·ªõi', 'c√≤n', 'ch·ªâ', 'm·ªõi', 'v·ª´a',
    'n√™n', 'ph·∫£i', 'c·∫ßn', 'n√™n', 'ƒë∆∞·ª£c', 'b·ªã', 'c√≥ th·ªÉ', 'kh√¥ng th·ªÉ'
}

def fallback_tokenize(text: str) -> List[str]:
    if not text:
        return []
    text = text.lower().strip()

    return re.findall(
        r'[a-zA-Z√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ'
        r'√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ'
        r'·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]+', text
    )
_vietnamese_tokenizer = None
_tokenizer_initialized = False
def get_vietnamese_tokenizer() -> Optional[Callable[[str], List[str]]]:
    """
    T·∫°o Vietnamese tokenizer s·ª≠ d·ª•ng underthesea
    Tr·∫£ v·ªÅ c·∫£ t·ª´ ƒë∆°n v√† c·ª•m t·ª´ c√≥ nghƒ©a (phrases)
    Returns None n·∫øu kh√¥ng c√†i ƒë·∫∑t ƒë∆∞·ª£c, s·∫Ω fallback v·ªÅ simple tokenizer
    """
    global _vietnamese_tokenizer, _tokenizer_initialized

    if _tokenizer_initialized:
        return _vietnamese_tokenizer
    
    _tokenizer_initialized = True
    try:
        from underthesea import word_tokenize
        logger.info("‚úÖ S·ª≠ d·ª•ng underthesea cho Vietnamese tokenization v·ªõi phrase extraction")
        
        cleaner = get_text_cleaner()

        def vietnamese_tokenize(text: str) -> List[str]:
            """Tokenize ti·∫øng Vi·ªát v·ªõi underthesea v√† t·∫°o c·ª•m t·ª´ c√≥ nghƒ©a"""
            if not text or not isinstance(text, str):
                return []
            
            # PREPROCESSING v·ªõi TextCleaner
            if cleaner:
                # Deep clean: normalize diacritics, expand abbreviations, lowercase
                try:
                    text = cleaner.clean(text, deep_clean=True, tokenize=False)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è L·ªói khi s·ª≠ d·ª•ng TextCleaner: {e}")
                    return fallback_tokenize(text)
            else:
                # Fallback: simple normalize
                text = text.lower().strip()
            
            if not text:
                return []
            
            try:
                # Word segmentation v·ªõi underthesea
                tokens = word_tokenize(text, format="text")
                # Split th√†nh list v√† filter (gi·ªØ nguy√™n d·∫•u _ ƒë·ªÉ gi·ªØ phrases)
                words = tokens.split()
                
                # Filter: lo·∫°i b·ªè stopwords, s·ªë, k√Ω t·ª± ƒë·∫∑c bi·ªát
                filtered_words = []
                for word in words:
                    # Lo·∫°i b·ªè stopwords
                    if word in VIETNAMESE_STOPWORDS:
                        continue
                    
                    # Lo·∫°i b·ªè s·ªë thu·∫ßn t√∫y
                    if word.isdigit():
                        continue
                    
                    # Lo·∫°i b·ªè t·ª´ qu√° ng·∫Øn (< 2 k√Ω t·ª±) ho·∫∑c qu√° d√†i (> 30)
                    if len(word) < 2 or len(word) > 30:
                        continue
                    
                    # Lo·∫°i b·ªè t·ª´ ch·ªâ c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát
                    if not re.search(r'[a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', word, re.I):
                        continue
                    
                    filtered_words.append(word)
                
                # T·∫°o c·ª•m t·ª´ c√≥ nghƒ©a (phrases) t·ª´ c√°c t·ª´ ƒë√£ filter
                # N·ªëi c√°c t·ª´ li√™n ti·∫øp kh√¥ng ph·∫£i stopwords th√†nh c·ª•m t·ª´
                phrases = list(filtered_words)
                
                # Th√™m t·ª´ ƒë∆°n
                #phrases.extend(filtered_words)
                
                # T·∫°o bigrams (2 t·ª´)
                for i in range(len(filtered_words) - 1):
                    bigram = f"{filtered_words[i]} {filtered_words[i+1]}"
                    # Ch·ªâ th√™m bigram n·∫øu kh√¥ng qu√° d√†i v√† c√≥ nghƒ©a
                    if len(bigram) <= 40:
                        phrases.append(bigram)
                
                # T·∫°o trigrams (3 t·ª´) cho c√°c c·ª•m t·ª´ ph·ªï bi·∫øn
                for i in range(len(filtered_words) - 2):
                    trigram = f"{filtered_words[i]} {filtered_words[i+1]} {filtered_words[i+2]}"
                    # Ch·ªâ th√™m trigram n·∫øu kh√¥ng qu√° d√†i
                    if len(trigram) <= 50:
                        phrases.append(trigram)
                
                return phrases
                
            except Exception as e:
                logger.warning(f"L·ªói khi tokenize v·ªõi underthesea: {e}, fallback v·ªÅ simple tokenizer")
                return fallback_tokenize(text)
        
        _vietnamese_tokenizer = vietnamese_tokenize
        return _vietnamese_tokenizer
        
    except ImportError:
        logger.warning("‚ö†Ô∏è underthesea ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. S·ª≠ d·ª•ng simple tokenizer.")
        logger.warning("üí° C√†i ƒë·∫∑t: pip install underthesea")
        _vietnamese_tokenizer = None
        return None
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o underthesea: {e}. S·ª≠ d·ª•ng simple tokenizer.")
        _vietnamese_tokenizer = None
        return None


def simple_vietnamese_tokenize(text: str) -> List[str]:
    """
    Simple Vietnamese tokenizer fallback
    T√°ch t·ª´ ƒë∆°n gi·∫£n d·ª±a tr√™n kho·∫£ng tr·∫Øng v√† regex
    """
    if not text or not isinstance(text, str):
        return []
    
    # PREPROCESSING v·ªõi TextCleaner
    cleaner = get_text_cleaner()
    if cleaner:
        text = cleaner.clean(text, deep_clean=True, tokenize=False)
    else:
        text = text.lower().strip()
    
    if not text:
        return []
    
    # T√°ch t·ª´ theo kho·∫£ng tr·∫Øng v√† d·∫•u c√¢u
    # Gi·ªØ l·∫°i c√°c t·ª´ ti·∫øng Vi·ªát (c√≥ d·∫•u) v√† ti·∫øng Anh
    words = re.findall(
        r'[a-zA-Z√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ'
        r'√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ'
        r'·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]+', text
    )
    
    # Filter
    filtered_words = []
    for word in words:
        # Lo·∫°i b·ªè stopwords
        if word in VIETNAMESE_STOPWORDS:
            continue
        
        # Lo·∫°i b·ªè s·ªë
        if word.isdigit():
            continue
        
        # Lo·∫°i b·ªè t·ª´ qu√° ng·∫Øn ho·∫∑c qu√° d√†i
        if len(word) < 2 or len(word) > 30:
            continue
        
        filtered_words.append(word)
    return filtered_words


def create_vietnamese_vectorizer_tokenizer():
    """
    T·∫°o tokenizer function cho CountVectorizer
    Returns callable function ho·∫∑c None
    """
    tokenizer = get_vietnamese_tokenizer()
    
    return tokenizer if tokenizer else fallback_tokenize