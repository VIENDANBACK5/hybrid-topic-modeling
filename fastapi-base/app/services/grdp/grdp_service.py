"""
GRDP Extraction Service - Structured Data Extraction Pipeline

Architecture (CHUáº¨N CÃ”NG NGHIá»†P):
1. Text Normalization - Chuáº©n hÃ³a sá»‘, Ä‘Æ¡n vá»‹
2. Chunking - Theo ngá»¯ nghÄ©a (tiÃªu Ä‘á», Ä‘oáº¡n)
3. Candidate Retrieval - BM25 tÃ¬m chunks liÃªn quan
4. LLM Extraction - Schema-guided JSON extraction
5. Validation - Rules engine chá»‘ng hallucination
6. Fill DB - Upsert vá»›i conflict resolution

âš ï¸ LLM KHÃ”NG pháº£i Ä‘á»ƒ search â€” mÃ  Ä‘á»ƒ EXTRACT structured data
"""
import re
import os
import json
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any, Tuple
from collections import Counter
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func, desc, text

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from app.models.model_article import Article
from app.models.model_economic_indicators import EconomicIndicator
from app.models.model_grdp_detail import GRDPDetail

logger = logging.getLogger(__name__)


# ========================================
# STEP 1: TEXT NORMALIZATION
# ========================================

def normalize_text(text: str) -> str:
    """
    Chuáº©n hÃ³a vÄƒn báº£n:
    - Sá»‘: 58.123 â†’ 58123, 8,2 â†’ 8.2
    - ÄÆ¡n vá»‹: thá»‘ng nháº¥t
    - Loáº¡i bá» rÃ¡c (header, footer, quáº£ng cÃ¡o)
    """
    if not text:
        return ""
    
    # Loáº¡i bá» URLs
    text = re.sub(r'https?://\S+|www\.\S+', ' ', text)
    
    # Loáº¡i bá» hashtags, mentions
    text = re.sub(r'#\w+|@\w+', ' ', text)
    
    # Chuáº©n hÃ³a sá»‘ cÃ³ dáº¥u cháº¥m ngÄƒn cÃ¡ch hÃ ng nghÃ¬n: 58.123 â†’ 58123
    text = re.sub(r'(\d{1,3})\.(\d{3})(?:\.(\d{3}))?(?!\d)', 
                  lambda m: m.group(1) + m.group(2) + (m.group(3) or ''), text)
    
    # Chuáº©n hÃ³a sá»‘ tháº­p phÃ¢n: 8,2% â†’ 8.2%
    text = re.sub(r'(\d+),(\d+)', r'\1.\2', text)
    
    # Chuáº©n hÃ³a Ä‘Æ¡n vá»‹
    text = re.sub(r'tá»·\s*(?:VNÄ|Ä‘á»“ng|vnÄ‘)', 'tá»· Ä‘á»“ng', text, flags=re.IGNORECASE)
    text = re.sub(r'triá»‡u\s*(?:VNÄ|Ä‘á»“ng|vnÄ‘)', 'triá»‡u Ä‘á»“ng', text, flags=re.IGNORECASE)
    
    # Chuáº©n hÃ³a GRDP/GADP
    text = re.sub(r'\bGADP\b', 'GRDP', text, flags=re.IGNORECASE)
    
    # Loáº¡i bá» multiple spaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


# ========================================
# STEP 2: SEMANTIC CHUNKING
# ========================================

def chunk_by_semantic(text: str) -> List[Dict[str, Any]]:
    """
    Chunk theo ngá»¯ nghÄ©a, khÃ´ng pháº£i random tokens.
    Æ¯u tiÃªn cÃ¡c Ä‘oáº¡n cÃ³ chá»©a keyword kinh táº¿.
    """
    chunks = []
    
    # Split theo Ä‘oáº¡n (2+ newlines hoáº·c .)
    paragraphs = re.split(r'\n\s*\n|(?<=[.!?])\s+(?=[A-ZÃ€Ãáº¢Ãƒáº ])', text)
    
    # Keywords cho GRDP
    grdp_keywords = [
        'grdp', 'gdp', 'tá»•ng sáº£n pháº©m', 'quy mÃ´ kinh táº¿',
        'tÄƒng trÆ°á»Ÿng', 'growth', 'bÃ¬nh quÃ¢n Ä‘áº§u ngÆ°á»i',
        'cÆ¡ cáº¥u kinh táº¿', 'nÃ´ng nghiá»‡p', 'cÃ´ng nghiá»‡p', 'dá»‹ch vá»¥',
        'giÃ¡ hiá»‡n hÃ nh', 'giÃ¡ so sÃ¡nh'
    ]
    
    for i, para in enumerate(paragraphs):
        para = para.strip()
        if len(para) < 20:  # Bá» Ä‘oáº¡n quÃ¡ ngáº¯n
            continue
        
        # TÃ­nh relevance score
        para_lower = para.lower()
        keyword_count = sum(1 for kw in grdp_keywords if kw in para_lower)
        
        # CÃ³ sá»‘ liá»‡u khÃ´ng?
        has_numbers = bool(re.search(r'\d+(?:\.\d+)?(?:\s*%|\s*tá»·|\s*triá»‡u)', para))
        
        chunks.append({
            'text': para,
            'index': i,
            'keyword_score': keyword_count,
            'has_numbers': has_numbers,
            'relevance': keyword_count * 2 + (3 if has_numbers else 0)
        })
    
    # Sort theo relevance giáº£m dáº§n
    chunks.sort(key=lambda x: x['relevance'], reverse=True)
    
    return chunks


# ========================================
# STEP 3: CANDIDATE RETRIEVAL (BM25)
# ========================================

def retrieve_grdp_candidates(db: Session, province: str, year: int, limit: int = 20) -> List[Dict]:
    """
    BM25-style search: TÃ¬m articles cÃ³ kháº£ nÄƒng chá»©a GRDP data.
    KhÃ´ng dÃ¹ng vector, chá»‰ keyword matching.
    """
    # TÃ­nh date range: year Â± 6 thÃ¡ng
    start_ts = datetime(year - 1, 7, 1).timestamp()
    end_ts = datetime(year + 1, 6, 30).timestamp()
    
    # Query vá»›i ILIKE (case-insensitive)
    query = db.query(Article).filter(
        and_(
            Article.published_date >= start_ts,
            Article.published_date <= end_ts,
            or_(
                Article.content.ilike(f'%{province}%'),
                Article.title.ilike(f'%{province}%')
            ),
            or_(
                Article.content.ilike('%GRDP%'),
                Article.content.ilike('%GADP%'),
                Article.content.ilike('%GDP%'),
                Article.content.ilike('%tá»•ng sáº£n pháº©m%'),
                Article.content.ilike('%tÄƒng trÆ°á»Ÿng%'),
                Article.content.ilike('%quy mÃ´ kinh táº¿%')
            )
        )
    ).order_by(desc(Article.published_date)).limit(limit)
    
    articles = query.all()
    
    candidates = []
    for article in articles:
        # Normalize text
        content = normalize_text(f"{article.title}\n\n{article.content}")
        
        # Chunk theo semantic
        chunks = chunk_by_semantic(content)
        
        # Láº¥y top chunks cÃ³ relevance cao
        top_chunks = [c for c in chunks if c['relevance'] >= 3][:5]
        
        if top_chunks:
            candidates.append({
                'article_id': article.id,
                'url': article.url,
                'title': article.title,
                'chunks': top_chunks,
                'total_relevance': sum(c['relevance'] for c in top_chunks)
            })
    
    # Sort theo total_relevance
    candidates.sort(key=lambda x: x['total_relevance'], reverse=True)
    
    logger.info(f"ğŸ“š Retrieved {len(candidates)} candidate articles with {sum(len(c['chunks']) for c in candidates)} chunks")
    
    return candidates


# ========================================
# STEP 4: LLM EXTRACTION (Schema-guided)
# ========================================

EXTRACTION_PROMPT = """Báº¡n lÃ  chuyÃªn gia trÃ­ch xuáº¥t dá»¯ liá»‡u kinh táº¿ Viá»‡t Nam.

NHIá»†M Vá»¤: TrÃ­ch xuáº¥t dá»¯ liá»‡u GRDP tá»« vÄƒn báº£n vÃ o JSON schema.

QUY Táº®C Báº®T BUá»˜C:
1. CHá»ˆ trÃ­ch xuáº¥t sá»‘ liá»‡u XUáº¤T HIá»†N RÃ• RÃ€NG trong vÄƒn báº£n
2. KHÃ”NG Æ°á»›c tÃ­nh, suy Ä‘oÃ¡n, tÃ­nh toÃ¡n
3. Náº¿u khÃ´ng tÃ¬m tháº¥y â†’ tráº£ vá» null
4. Sá»‘ liá»‡u pháº£i khá»›p vá»›i tá»‰nh {province} vÃ  nÄƒm {year}
5. ÄÆ¡n vá»‹: GRDP = tá»· Ä‘á»“ng, bÃ¬nh quÃ¢n = triá»‡u Ä‘á»“ng, tá»· trá»ng = %

SCHEMA OUTPUT (JSON):
{{
  "province": "string - tÃªn tá»‰nh",
  "year": number,
  "quarter": number hoáº·c null,
  "grdp_current_price": number hoáº·c null (tá»· Ä‘á»“ng),
  "grdp_per_capita": number hoáº·c null (triá»‡u Ä‘á»“ng),
  "growth_rate": number hoáº·c null (%),
  "agriculture_sector_pct": number hoáº·c null (%),
  "industry_sector_pct": number hoáº·c null (%),
  "service_sector_pct": number hoáº·c null (%)
}}

VÄ‚N Báº¢N Cáº¦N TRÃCH XUáº¤T:
<<<
{text}
>>>

Tráº£ vá» CHÃNH XÃC 1 JSON object. KhÃ´ng giáº£i thÃ­ch."""


class GRDPLLMExtractor:
    """LLM-based structured data extraction"""
    
    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found")
        
        self.llm = ChatOpenAI(
            model="openai/gpt-4o-mini",
            temperature=0,  # Deterministic
            openai_api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )
    
    def extract(self, text: str, province: str, year: int) -> Optional[Dict]:
        """Extract GRDP data tá»« text chunk"""
        try:
            prompt = EXTRACTION_PROMPT.format(
                province=province,
                year=year,
                text=text
            )
            
            result = self.llm.invoke(prompt)
            content = result.content.strip()
            
            # Parse JSON - tÃ¬m JSON trong response
            json_match = re.search(r'\{[^{}]*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                return data
            
            return None
            
        except Exception as e:
            logger.error(f"LLM extraction error: {e}")
            return None


# ========================================
# STEP 5: VALIDATION & RULES ENGINE
# ========================================

def validate_grdp_data(data: Dict) -> Tuple[bool, List[str], str]:
    """
    Validate extracted data vá»›i business rules.
    
    Returns:
        (is_valid, errors, data_status)
    """
    errors = []
    data_status = "official"
    
    if not data:
        return False, ["Empty data"], "invalid"
    
    # Rule 1: Year pháº£i há»£p lá»‡
    year = data.get('year')
    if year and not (2000 <= year <= 2030):
        errors.append(f"Year {year} out of range [2000-2030]")
    
    # Rule 2: Growth rate pháº£i há»£p lÃ½ (-20% to 30%)
    growth = data.get('growth_rate')
    if growth is not None:
        if not (-20 <= growth <= 30):
            errors.append(f"Growth rate {growth}% unrealistic")
            data_status = "estimated"
    
    # Rule 3: GRDP pháº£i dÆ°Æ¡ng
    grdp = data.get('grdp_current_price')
    if grdp is not None and grdp <= 0:
        errors.append(f"GRDP {grdp} must be positive")
    
    # Rule 4: Tá»•ng cÆ¡ cáº¥u ngÃ nh â‰ˆ 100%
    sectors = [
        data.get('agriculture_sector_pct'),
        data.get('industry_sector_pct'),
        data.get('service_sector_pct')
    ]
    valid_sectors = [s for s in sectors if s is not None]
    if len(valid_sectors) == 3:
        total = sum(valid_sectors)
        if not (95 <= total <= 105):
            errors.append(f"Sector sum {total}% not â‰ˆ 100%")
            data_status = "estimated"
    
    # Rule 5: Per capita há»£p lÃ½ (10-500 triá»‡u Ä‘á»“ng)
    per_capita = data.get('grdp_per_capita')
    if per_capita is not None:
        if not (10 <= per_capita <= 500):
            errors.append(f"Per capita {per_capita} triá»‡u unrealistic")
            data_status = "estimated"
    
    # Rule 6: GRDP HÆ°ng YÃªn ~50,000-200,000 tá»· Ä‘á»“ng
    if grdp is not None and data.get('province') == 'HÆ°ng YÃªn':
        if not (30000 <= grdp <= 300000):
            errors.append(f"GRDP {grdp} tá»· unrealistic for HÆ°ng YÃªn")
            data_status = "estimated"
    
    is_valid = len(errors) == 0
    
    if errors:
        logger.warning(f"âš ï¸ Validation warnings: {errors}")
    
    return is_valid, errors, data_status


def merge_extractions(extractions: List[Dict]) -> Dict:
    """
    Merge nhiá»u extractions tá»« cÃ¡c chunks khÃ¡c nhau.
    Æ¯u tiÃªn: giÃ¡ trá»‹ xuáº¥t hiá»‡n nhiá»u nháº¥t (voting).
    """
    if not extractions:
        return {}
    
    if len(extractions) == 1:
        return extractions[0]
    
    # Merge field by field
    merged = {}
    fields = [
        'province', 'year', 'quarter',
        'grdp_current_price', 'grdp_per_capita', 'growth_rate',
        'agriculture_sector_pct', 'industry_sector_pct', 'service_sector_pct'
    ]
    
    for field in fields:
        values = [e.get(field) for e in extractions if e.get(field) is not None]
        if values:
            # Láº¥y giÃ¡ trá»‹ xuáº¥t hiá»‡n nhiá»u nháº¥t (voting)
            counter = Counter(values)
            merged[field] = counter.most_common(1)[0][0]
    
    return merged


# ========================================
# STEP 6: MAIN EXTRACTOR SERVICE
# ========================================

class GRDPDataExtractor:
    """
    Main orchestrator cho GRDP extraction pipeline:
    1. Retrieve candidates (BM25)
    2. Chunk semantic
    3. LLM extract (schema-guided)
    4. Validate
    5. Fill DB
    """
    
    PROVINCE = "HÆ°ng YÃªn"
    
    def __init__(self, db: Session):
        self.db = db
        self.province = self.PROVINCE
        self.llm_extractor = None
    
    def _get_llm_extractor(self):
        """Lazy init LLM extractor"""
        if self.llm_extractor is None:
            self.llm_extractor = GRDPLLMExtractor()
        return self.llm_extractor
    
    def extract_grdp_from_articles(self, year: int, quarter: Optional[int] = None, use_llm: bool = True) -> Optional[Dict]:
        """
        Pipeline chÃ­nh:
        1. Retrieve candidate articles
        2. Extract tá»« chunks báº±ng LLM
        3. Merge & validate
        """
        logger.info(f"ğŸ¯ Extracting GRDP: {self.province} - {year}")
        
        # Step 1: Retrieve candidates
        candidates = retrieve_grdp_candidates(self.db, self.province, year, limit=20)
        
        if not candidates:
            logger.info("âŒ No candidate articles found")
            return None
        
        logger.info(f"ğŸ“š Found {len(candidates)} candidate articles")
        
        # Step 2-3: Extract tá»« tá»«ng chunk
        all_extractions = []
        sources = []
        
        for cand in candidates[:10]:  # Top 10 articles
            article_extractions = []
            
            for chunk in cand['chunks'][:3]:  # Top 3 chunks per article
                if use_llm:
                    # LLM extraction
                    extractor = self._get_llm_extractor()
                    extracted = extractor.extract(
                        text=chunk['text'],
                        province=self.province,
                        year=year
                    )
                else:
                    # Regex fallback
                    extracted = self._regex_extract(chunk['text'], year)
                
                if extracted and any(v is not None for k, v in extracted.items() if k not in ['province', 'year', 'quarter']):
                    article_extractions.append(extracted)
                    logger.info(f"  âœ“ Extracted from article {cand['article_id']}: {chunk['text'][:50]}...")
            
            if article_extractions:
                sources.append(cand['url'])
                all_extractions.extend(article_extractions)
        
        if not all_extractions:
            logger.info("âŒ No data extracted from any chunks")
            return None
        
        logger.info(f"ğŸ“Š Total {len(all_extractions)} extractions from {len(sources)} articles")
        
        # Step 4: Merge extractions
        merged = merge_extractions(all_extractions)
        
        # Ensure required fields
        merged['province'] = self.province
        merged['year'] = year
        merged['quarter'] = quarter
        
        # Step 5: Validate
        is_valid, errors, data_status = validate_grdp_data(merged)
        merged['data_status'] = data_status
        merged['data_source'] = ' + '.join(sources[:3])
        
        logger.info(f"âœ… Merged result: GRDP={merged.get('grdp_current_price')}, Growth={merged.get('growth_rate')}%")
        
        return merged
    
    def _regex_extract(self, text: str, year: int) -> Dict:
        """Regex-based extraction (fallback khi khÃ´ng dÃ¹ng LLM)"""
        result = {
            'province': self.province,
            'year': year,
            'quarter': None
        }
        
        text_lower = text.lower()
        
        # GRDP value (tÃ¬m sá»‘ lá»›n + "tá»·")
        patterns = [
            r'grdp.*?(?:Ä‘áº¡t|Æ°á»›c Ä‘áº¡t|lÃ )\s*(\d+(?:\.\d+)?)\s*tá»·',
            r'tá»•ng sáº£n pháº©m.*?(\d+(?:\.\d+)?)\s*tá»·',
            r'(\d{4,6})\s*tá»·.*?(?:grdp|tá»•ng sáº£n pháº©m)',
        ]
        for p in patterns:
            m = re.search(p, text_lower)
            if m:
                try:
                    result['grdp_current_price'] = float(m.group(1))
                    break
                except:
                    pass
        
        # Growth rate
        m = re.search(r'tÄƒng(?:\s+trÆ°á»Ÿng)?.*?(\d+(?:\.\d+)?)\s*%', text_lower)
        if m:
            try:
                result['growth_rate'] = float(m.group(1))
            except:
                pass
        
        # Per capita
        m = re.search(r'bÃ¬nh quÃ¢n.*?(\d+(?:\.\d+)?)\s*triá»‡u', text_lower)
        if m:
            try:
                result['grdp_per_capita'] = float(m.group(1))
            except:
                pass
        
        # Sectors
        sectors = {
            'agriculture_sector_pct': r'nÃ´ng nghiá»‡p.*?(\d+(?:\.\d+)?)\s*%',
            'industry_sector_pct': r'cÃ´ng nghiá»‡p.*?(\d+(?:\.\d+)?)\s*%',
            'service_sector_pct': r'dá»‹ch vá»¥.*?(\d+(?:\.\d+)?)\s*%'
        }
        for field, pattern in sectors.items():
            m = re.search(pattern, text_lower)
            if m:
                try:
                    result[field] = float(m.group(1))
                except:
                    pass
        
        return result
    
    def extract_grdp_from_economic_indicators(self, year: int, quarter: Optional[int] = None) -> Optional[Dict]:
        """Extract tá»« báº£ng economic_indicators (Ä‘Ã£ cÃ³ sáºµn)"""
        try:
            query = self.db.query(EconomicIndicator).filter(
                EconomicIndicator.province == self.province,
                EconomicIndicator.year == year
            )
            
            if quarter:
                query = query.filter(EconomicIndicator.quarter == quarter)
            
            indicator = query.first()
            
            if not indicator:
                return None
            
            result = {
                'province': self.province,
                'year': year,
                'quarter': quarter,
                'data_source': 'economic_indicators table',
                'data_status': 'official'
            }
            
            # Map fields
            field_map = {
                'grdp_current_price': 'grdp_current_price',
                'grdp_per_capita': 'grdp_per_capita',
                'grdp_growth_rate': 'growth_rate'
            }
            
            for src, dst in field_map.items():
                if hasattr(indicator, src):
                    val = getattr(indicator, src)
                    if val is not None:
                        result[dst] = val
            
            return result
            
        except Exception as e:
            logger.error(f"Error reading economic_indicators: {e}")
            return None
    
    def extract_grdp_data(self, year: int, quarter: Optional[int] = None, use_llm: bool = True) -> Optional[Dict]:
        """
        Main extraction: articles â†’ indicators â†’ LLM pure
        """
        # Priority 1: Articles
        logger.info("ğŸ“„ Step 1: Extracting from articles...")
        data = self.extract_grdp_from_articles(year, quarter, use_llm)
        if data:
            logger.info("âœ… Found in articles")
            return data
        
        # Priority 2: Economic Indicators table
        logger.info("ğŸ“Š Step 2: Checking economic_indicators table...")
        data = self.extract_grdp_from_economic_indicators(year, quarter)
        if data:
            logger.info("âœ… Found in economic_indicators")
            return data
        
        logger.info("âŒ No GRDP data found")
        return None
    
    def save_grdp_detail(self, data: Dict, force_update: bool = True) -> GRDPDetail:
        """Save/Update vÃ o DB vá»›i ON CONFLICT logic"""
        try:
            # Check existing
            query = self.db.query(GRDPDetail).filter(
                GRDPDetail.province == data['province'],
                GRDPDetail.year == data['year']
            )
            
            if data.get('quarter'):
                query = query.filter(GRDPDetail.quarter == data['quarter'])
            else:
                query = query.filter(GRDPDetail.quarter.is_(None))
            
            existing = query.first()
            
            # Clean data - chá»‰ giá»¯ fields há»£p lá»‡
            clean_fields = [
                'province', 'year', 'quarter',
                'grdp_current_price', 'grdp_per_capita', 'growth_rate',
                'agriculture_sector_pct', 'industry_sector_pct', 'service_sector_pct',
                'rank_national', 'forecast_year_end', 'data_status', 'data_source'
            ]
            clean_data = {k: data.get(k) for k in clean_fields if k in data}
            
            if existing:
                if force_update:
                    for key, value in clean_data.items():
                        if value is not None:
                            setattr(existing, key, value)
                    existing.last_updated = datetime.now()
                    self.db.commit()
                    self.db.refresh(existing)
                    logger.info(f"â™»ï¸ Updated GRDP id={existing.id}")
                    return existing
                else:
                    return existing
            else:
                new_record = GRDPDetail(**clean_data)
                self.db.add(new_record)
                self.db.commit()
                self.db.refresh(new_record)
                logger.info(f"âœ¨ Created GRDP id={new_record.id}")
                return new_record
                
        except Exception as e:
            logger.error(f"Save error: {e}")
            self.db.rollback()
            raise
    
    def get_or_extract_grdp(self, year: int, quarter: Optional[int] = None, use_llm: bool = True, force_update: bool = True) -> Optional[GRDPDetail]:
        """Wrapper: extract + save"""
        data = self.extract_grdp_data(year, quarter, use_llm)
        
        if not data:
            return None
        
        return self.save_grdp_detail(data, force_update)
