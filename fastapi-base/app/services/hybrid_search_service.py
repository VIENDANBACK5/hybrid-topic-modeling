"""
Hybrid Search Service - BM25 + Vector Search + RAG
Káº¿t há»£p full-text search vÃ  semantic search Ä‘á»ƒ tÃ¬m articles relevant
"""
import logging
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text, or_, and_, func, desc
from datetime import datetime, timedelta

from app.models.model_article import Article

logger = logging.getLogger(__name__)


class HybridSearchService:
    """
    Service tÃ¬m kiáº¿m hybrid: BM25 (PostgreSQL full-text) + Semantic context
    """
    
    def __init__(self, db: Session):
        self.db = db
    
    def search_articles_for_grdp(
        self,
        province: str,
        year: int,
        quarter: Optional[int] = None,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """
        TÃ¬m kiáº¿m articles vá» GRDP cá»§a tá»‰nh báº±ng hybrid approach
        
        Args:
            province: TÃªn tá»‰nh (VD: "HÆ°ng YÃªn")
            year: NÄƒm cáº§n tÃ¬m
            quarter: QuÃ½ (náº¿u cÃ³)
            top_k: Sá»‘ lÆ°á»£ng bÃ i viáº¿t top
        
        Returns:
            List cÃ¡c article dict cÃ³ score
        """
        
        # 1. BUILD SEARCH QUERY - BM25 style
        search_terms = self._build_search_terms(province, year, quarter)
        
        # 2. FULL-TEXT SEARCH vá»›i PostgreSQL
        results = self._bm25_search(search_terms, year, quarter, top_k * 3)  # Láº¥y nhiá»u hÆ¡n Ä‘á»ƒ filter
        
        # 3. SEMANTIC FILTERING - Lá»c theo context
        filtered_results = self._semantic_filter(results, province, year, quarter)
        
        # 4. RANKING - Xáº¿p háº¡ng theo relevance
        ranked_results = self._rank_by_relevance(filtered_results, province, year, quarter)
        
        return ranked_results[:top_k]
    
    def _build_search_terms(
        self, 
        province: str, 
        year: int, 
        quarter: Optional[int]
    ) -> List[str]:
        """
        Táº¡o cÃ¡c search terms cho BM25
        """
        terms = [
            province,
            f"nÄƒm {year}",
            "GRDP",
            "tá»•ng sáº£n pháº©m",
            "tÄƒng trÆ°á»Ÿng",
            "kinh táº¿",
            "GDP",
            "quy mÃ´",
            "phÃ¡t triá»ƒn",
            "bÃ¬nh quÃ¢n",
            "cÃ´ng nghiá»‡p",
            "nÃ´ng nghiá»‡p",
            "dá»‹ch vá»¥",
            "xuáº¥t kháº©u",
            "thu hÃºt Ä‘áº§u tÆ°",
        ]
        
        if quarter:
            terms.extend([
                f"quÃ½ {quarter}",
                f"Q{quarter}",
                f"{quarter} thÃ¡ng Ä‘áº§u nÄƒm"
            ])
        
        return terms
    
    def _bm25_search(
        self,
        search_terms: List[str],
        year: int,
        quarter: Optional[int],
        limit: int
    ) -> List[Article]:
        """
        Full-text search sá»­ dá»¥ng PostgreSQL ILIKE (simplified BM25)
        Trong production nÃªn dÃ¹ng tsvector + tsquery
        """
        
        try:
            # Time range: +/- 1 nÄƒm quanh nÄƒm cáº§n tÃ¬m (ná»›i lá»ng Ä‘á»ƒ tÃ¬m Ä‘Æ°á»£c nhiá»u articles)
            start_dt = datetime(year - 1, 1, 1)
            end_dt = datetime(year + 1, 12, 31, 23, 59, 59)
            
            start_timestamp = start_dt.timestamp()
            end_timestamp = end_dt.timestamp()
            
            logger.info(f"ğŸ“… Date range: {start_dt} â†’ {end_dt} (timestamps: {start_timestamp} â†’ {end_timestamp})")
            
            query = self.db.query(Article)
            
            # Filter by date range - published_date lÃ  Float (UNIX timestamp)
            query = query.filter(
                and_(
                    Article.published_date >= start_timestamp,
                    Article.published_date <= end_timestamp
                )
            )
            
            # Multi-term search
            conditions = []
            for term in search_terms[:5]:  # Top 5 important terms
                conditions.append(
                    or_(
                        Article.title.ilike(f"%{term}%"),
                        Article.content.ilike(f"%{term}%")
                    )
                )
            
            if conditions:
                query = query.filter(or_(*conditions))
            
            # Order by most recent and with longer content (proxy for quality)
            query = query.order_by(
                desc(func.length(Article.content)),
                desc(Article.published_date)  # UNIX timestamp - cÃ ng lá»›n cÃ ng má»›i
            )
            
            results = query.limit(limit).all()
            
            logger.info(f"ğŸ” BM25 search found {len(results)} articles")
            return results
            
        except Exception as e:
            logger.error(f"BM25 search error: {str(e)}")
            self.db.rollback()
            return []
    
    def _semantic_filter(
        self,
        articles: List[Article],
        province: str,
        year: int,
        quarter: Optional[int]
    ) -> List[Dict[str, Any]]:
        """
        Filter bÃ i viáº¿t theo semantic context:
        - CÃ³ chá»©a tÃªn tá»‰nh
        - CÃ³ chá»©a nÄƒm/quÃ½
        - CÃ³ chá»©a chá»‰ sá»‘ kinh táº¿
        """
        
        filtered = []
        
        for article in articles:
            text = f"{article.title} {article.content}".lower()
            
            # Check 1: CÃ³ chá»©a tÃªn tá»‰nh
            if province.lower() not in text:
                continue
            
            # Check 2: CÃ³ chá»©a nÄƒm
            year_patterns = [
                f"nÄƒm {year}",
                f"{year}",
                f"nÄƒm {year - 1}",  # CÃ³ thá»ƒ so sÃ¡nh vá»›i nÄƒm trÆ°á»›c
            ]
            has_year = any(pattern in text for pattern in year_patterns)
            if not has_year:
                continue
            
            # Check 3: CÃ³ chá»©a chá»‰ sá»‘ kinh táº¿
            economic_keywords = [
                "grdp", "gdp", "tá»•ng sáº£n pháº©m", "tÄƒng trÆ°á»Ÿng", 
                "tá»· Ä‘á»“ng", "nghÃ¬n tá»·", "triá»‡u Ä‘á»“ng",
                "phÃ¡t triá»ƒn kinh táº¿", "thu hÃºt Ä‘áº§u tÆ°",
                "xuáº¥t kháº©u", "cÃ´ng nghiá»‡p", "nÃ´ng nghiá»‡p"
            ]
            has_economic = any(kw in text for kw in economic_keywords)
            if not has_economic:
                continue
            
            # Calculate relevance score
            score = self._calculate_score(article, province, year, quarter)
            
            filtered.append({
                "article": article,
                "score": score,
                "has_numbers": self._has_numeric_data(text)
            })
        
        logger.info(f"âœ… Semantic filter kept {len(filtered)}/{len(articles)} articles")
        return filtered
    
    def _calculate_score(
        self,
        article: Article,
        province: str,
        year: int,
        quarter: Optional[int]
    ) -> float:
        """
        TÃ­nh relevance score cho article
        """
        text = f"{article.title} {article.content}".lower()
        score = 0.0
        
        # Title bonus
        if province.lower() in article.title.lower():
            score += 2.0
        
        # Exact year match
        if f"nÄƒm {year}" in text:
            score += 3.0
        
        # Quarter match
        if quarter and f"quÃ½ {quarter}" in text:
            score += 2.0
        
        # Keyword density
        keywords = ["grdp", "tÄƒng trÆ°á»Ÿng", "tá»· Ä‘á»“ng", "phÃ¡t triá»ƒn"]
        for kw in keywords:
            score += text.count(kw) * 0.5
        
        # Content length (longer = more detailed)
        if len(article.content) > 2000:
            score += 1.0
        
        # Recency (newer articles might have updated data)
        if article.published_date:
            article_date = datetime.fromtimestamp(article.published_date)
            days_diff = abs((article_date - datetime(year, 6, 30)).days)
            if days_diff < 180:  # Within 6 months
                score += 1.0
        
        return score
    
    def _has_numeric_data(self, text: str) -> bool:
        """
        Kiá»ƒm tra cÃ³ chá»©a sá»‘ liá»‡u khÃ´ng
        """
        numeric_patterns = [
            r'\d+[\.,]?\d*\s*tá»·',
            r'\d+[\.,]?\d*%',
            r'\d+[\.,]?\d*\s*triá»‡u'
        ]
        import re
        return any(re.search(pattern, text) for pattern in numeric_patterns)
    
    def _rank_by_relevance(
        self,
        results: List[Dict[str, Any]],
        province: str,
        year: int,
        quarter: Optional[int]
    ) -> List[Dict[str, Any]]:
        """
        Xáº¿p háº¡ng theo score vÃ  cÃ³ sá»‘ liá»‡u
        """
        # Sort by score descending, then by has_numbers
        sorted_results = sorted(
            results,
            key=lambda x: (x["score"], x["has_numbers"]),
            reverse=True
        )
        
        logger.info(f"ğŸ“Š Top article score: {sorted_results[0]['score']:.2f}" if sorted_results else "No results")
        return sorted_results
    
    def prepare_context_for_llm(
        self,
        articles: List[Dict[str, Any]],
        max_tokens: int = 6000
    ) -> str:
        """
        Chuáº©n bá»‹ context tá»« top articles Ä‘á»ƒ pass vÃ o LLM
        Giá»›i háº¡n token Ä‘á»ƒ fit vÃ o context window
        """
        
        context_parts = []
        current_length = 0
        max_length = max_tokens * 3  # Rough estimate: 1 token â‰ˆ 3 chars Vietnamese
        
        for i, item in enumerate(articles[:5], 1):  # Top 5 articles
            article = item["article"]
            
            # Extract relevant snippet (first 1500 chars + last 500 chars)
            content = article.content[:1500]
            if len(article.content) > 2000:
                content += "..." + article.content[-500:]
            
            published_str = 'N/A'
            if article.published_date:
                try:
                    published_str = datetime.fromtimestamp(article.published_date).strftime('%d/%m/%Y')
                except:
                    pass
            
            snippet = f"""
[Nguá»“n {i}] {article.title}
URL: {article.url}
NgÃ y: {published_str}
Relevance Score: {item['score']:.2f}

{content}

---
"""
            
            if current_length + len(snippet) > max_length:
                break
            
            context_parts.append(snippet)
            current_length += len(snippet)
        
        context = "\n".join(context_parts)
        
        logger.info(f"ğŸ“ Prepared context: {len(context)} chars from {len(context_parts)} articles")
        return context
